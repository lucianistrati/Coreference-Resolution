{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gap-coreference', 'sample', 'coref-by-mlp-cnn-coattention']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/gap-coreference/gap-test.tsv',sep=\"\\t\")\n",
    "test_df = pd.read_csv('../input/gap-coreference/gap-development.tsv',sep=\"\\t\")\n",
    "dev_df = pd.read_csv('../input/gap-coreference/gap-validation.tsv',sep=\"\\t\")\n",
    "\n",
    "#pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Upon their acceptance into the Kontinental Hockey League, Dehner left Finland to sign a contract in Germany with EHC M*nchen of the DEL on June 18, 2014. After capturing the German championship with the M*nchen team in 2016, he left the club and was picked up by fellow DEL side EHC Wolfsburg in July 2016. Former NHLer Gary Suter and Olympic-medalist Bob Suter are Dehner's uncles. His cousin is Minnesota Wild's alternate captain Ryan Suter.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"Text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Between the years 1979-1981, River won four local titles, and became one of the most expensive teams in the world, with a first team (Alonso- Luque) playing in league games and an equally prestigious second team (Carrasco- Ram*n D*az) used mostly in Copa Libertadores matches. During the 1981 ``Nacional'' tournament (which River would eventually win), Alonso often clashed with then coach Alfredo Di St*fano (who seldom selected him for the first team and instead put younger players such as Carlos Daniel Tapia and Jose Maria Vieta in his position).\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"Text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>validation-1</td>\n",
       "      <td>He admitted making four trips to China and pla...</td>\n",
       "      <td>him</td>\n",
       "      <td>256</td>\n",
       "      <td>Jose de Venecia Jr</td>\n",
       "      <td>208</td>\n",
       "      <td>False</td>\n",
       "      <td>Abalos</td>\n",
       "      <td>241</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Commission_on_Ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>validation-2</td>\n",
       "      <td>Kathleen Nott was born in Camberwell, London. ...</td>\n",
       "      <td>She</td>\n",
       "      <td>185</td>\n",
       "      <td>Ellen</td>\n",
       "      <td>110</td>\n",
       "      <td>False</td>\n",
       "      <td>Kathleen</td>\n",
       "      <td>150</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kathleen_Nott</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>validation-3</td>\n",
       "      <td>When she returns to her hotel room, a Liberian...</td>\n",
       "      <td>his</td>\n",
       "      <td>435</td>\n",
       "      <td>Jason Scott Lee</td>\n",
       "      <td>383</td>\n",
       "      <td>False</td>\n",
       "      <td>Danny</td>\n",
       "      <td>406</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Hawaii_Five-0_(20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>validation-4</td>\n",
       "      <td>On 19 March 2007, during a campaign appearance...</td>\n",
       "      <td>he</td>\n",
       "      <td>333</td>\n",
       "      <td>Reucassel</td>\n",
       "      <td>300</td>\n",
       "      <td>True</td>\n",
       "      <td>Debnam</td>\n",
       "      <td>325</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Craig_Reucassel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>validation-5</td>\n",
       "      <td>By this time, Karen Blixen had separated from ...</td>\n",
       "      <td>she</td>\n",
       "      <td>427</td>\n",
       "      <td>Finch Hatton</td>\n",
       "      <td>290</td>\n",
       "      <td>False</td>\n",
       "      <td>Beryl Markham</td>\n",
       "      <td>328</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Denys_Finch_Hatton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>validation-6</td>\n",
       "      <td>No amount of logic can shatter a faith conscio...</td>\n",
       "      <td>he</td>\n",
       "      <td>296</td>\n",
       "      <td>James Randi</td>\n",
       "      <td>189</td>\n",
       "      <td>False</td>\n",
       "      <td>Jos* Alvarez</td>\n",
       "      <td>272</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/True-believer_syn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>validation-7</td>\n",
       "      <td>Lieutenant General Weber Pasha wanted Faik Pas...</td>\n",
       "      <td>He</td>\n",
       "      <td>294</td>\n",
       "      <td>von Sanders</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "      <td>Faik Pasha</td>\n",
       "      <td>253</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Faik_Pasha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>validation-8</td>\n",
       "      <td>He went on to enter mainstream journalism as a...</td>\n",
       "      <td>his</td>\n",
       "      <td>378</td>\n",
       "      <td>Colin</td>\n",
       "      <td>267</td>\n",
       "      <td>False</td>\n",
       "      <td>Jake Burns</td>\n",
       "      <td>340</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Colin_McClelland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>validation-9</td>\n",
       "      <td>In 1940 Lester Cowan, an independent film prod...</td>\n",
       "      <td>he</td>\n",
       "      <td>399</td>\n",
       "      <td>Scott</td>\n",
       "      <td>370</td>\n",
       "      <td>False</td>\n",
       "      <td>Cowan</td>\n",
       "      <td>388</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Shirley_Temple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>validation-10</td>\n",
       "      <td>They have a stormy marriage, caused by his hot...</td>\n",
       "      <td>her</td>\n",
       "      <td>298</td>\n",
       "      <td>Beverley Callard</td>\n",
       "      <td>325</td>\n",
       "      <td>True</td>\n",
       "      <td>Liz</td>\n",
       "      <td>370</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Liz_McDonald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>validation-11</td>\n",
       "      <td>This particular government recalled all the Gr...</td>\n",
       "      <td>he</td>\n",
       "      <td>418</td>\n",
       "      <td>Ioannis Mamouris</td>\n",
       "      <td>273</td>\n",
       "      <td>False</td>\n",
       "      <td>Kallergis</td>\n",
       "      <td>435</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Dimitrios_Kallergis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>validation-12</td>\n",
       "      <td>Nicole flirts with Charlie and gives him her p...</td>\n",
       "      <td>her</td>\n",
       "      <td>266</td>\n",
       "      <td>Nicole</td>\n",
       "      <td>208</td>\n",
       "      <td>True</td>\n",
       "      <td>Annie Sobacz</td>\n",
       "      <td>231</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Brodie_family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>validation-13</td>\n",
       "      <td>In the past,Princess Luminous is killed by Que...</td>\n",
       "      <td>her</td>\n",
       "      <td>345</td>\n",
       "      <td>Queen</td>\n",
       "      <td>312</td>\n",
       "      <td>True</td>\n",
       "      <td>Crystal</td>\n",
       "      <td>330</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Futari_wa_Pretty_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>validation-14</td>\n",
       "      <td>It's Always Fair Weather is a 1955 MGM musical...</td>\n",
       "      <td>his</td>\n",
       "      <td>264</td>\n",
       "      <td>Dan Dailey</td>\n",
       "      <td>183</td>\n",
       "      <td>False</td>\n",
       "      <td>Michael Kidd</td>\n",
       "      <td>248</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/It's_Always_Fair_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>validation-15</td>\n",
       "      <td>Aviation historian Phil Scott in The Shoulders...</td>\n",
       "      <td>his</td>\n",
       "      <td>251</td>\n",
       "      <td>Scott</td>\n",
       "      <td>196</td>\n",
       "      <td>False</td>\n",
       "      <td>Herring</td>\n",
       "      <td>207</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Augustus_Moore_He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>validation-16</td>\n",
       "      <td>In spite of this, ``Sullivan took (Wright) und...</td>\n",
       "      <td>his</td>\n",
       "      <td>319</td>\n",
       "      <td>Wright</td>\n",
       "      <td>265</td>\n",
       "      <td>True</td>\n",
       "      <td>Mueller</td>\n",
       "      <td>291</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Frank_Lloyd_Wright</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>validation-17</td>\n",
       "      <td>Fripp has performed Soundscapes in several sit...</td>\n",
       "      <td>his</td>\n",
       "      <td>319</td>\n",
       "      <td>Steve Ball</td>\n",
       "      <td>195</td>\n",
       "      <td>False</td>\n",
       "      <td>Robert Fripp</td>\n",
       "      <td>214</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Soundscapes_by_Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>validation-18</td>\n",
       "      <td>In the event, the proletariat through the dict...</td>\n",
       "      <td>his</td>\n",
       "      <td>382</td>\n",
       "      <td>Marx</td>\n",
       "      <td>321</td>\n",
       "      <td>False</td>\n",
       "      <td>Lenin</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/The_State_and_Rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>validation-19</td>\n",
       "      <td>But that backfires when Andy decides to renege...</td>\n",
       "      <td>his</td>\n",
       "      <td>255</td>\n",
       "      <td>Dwight</td>\n",
       "      <td>164</td>\n",
       "      <td>False</td>\n",
       "      <td>Andy</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Andy_Bernard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>validation-20</td>\n",
       "      <td>Significant places listed individually on the ...</td>\n",
       "      <td>his</td>\n",
       "      <td>337</td>\n",
       "      <td>Morris</td>\n",
       "      <td>275</td>\n",
       "      <td>False</td>\n",
       "      <td>David W. Taylor</td>\n",
       "      <td>317</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Green_Springs_Nat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>validation-21</td>\n",
       "      <td>Thoughtful and quietly religious, Byron is sup...</td>\n",
       "      <td>his</td>\n",
       "      <td>346</td>\n",
       "      <td>Joe Brown</td>\n",
       "      <td>216</td>\n",
       "      <td>False</td>\n",
       "      <td>Joe Christmas</td>\n",
       "      <td>236</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Light_in_August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>validation-22</td>\n",
       "      <td>He also felt that Hicks could capture the ``gr...</td>\n",
       "      <td>she</td>\n",
       "      <td>306</td>\n",
       "      <td>Hicks</td>\n",
       "      <td>210</td>\n",
       "      <td>True</td>\n",
       "      <td>Ellie</td>\n",
       "      <td>278</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/The_Last_of_Us:_A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>validation-23</td>\n",
       "      <td>Bonavia's paintings share with Vernet's a roco...</td>\n",
       "      <td>his</td>\n",
       "      <td>387</td>\n",
       "      <td>Bonavia</td>\n",
       "      <td>310</td>\n",
       "      <td>True</td>\n",
       "      <td>Brudenell</td>\n",
       "      <td>409</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Carlo_Bonavia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>validation-24</td>\n",
       "      <td>Next up is Blond Baller vs Corn Fed; straight ...</td>\n",
       "      <td>she</td>\n",
       "      <td>379</td>\n",
       "      <td>Marcia</td>\n",
       "      <td>320</td>\n",
       "      <td>True</td>\n",
       "      <td>Mindy</td>\n",
       "      <td>339</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/I_Love_Money_(sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>validation-25</td>\n",
       "      <td>General Adkins served in the Maryland Military...</td>\n",
       "      <td>his</td>\n",
       "      <td>309</td>\n",
       "      <td>Martin O*Malley</td>\n",
       "      <td>265</td>\n",
       "      <td>True</td>\n",
       "      <td>Adkins</td>\n",
       "      <td>299</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/James_A._Adkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>validation-26</td>\n",
       "      <td>Ahmedabad was then reoccupied by the Muzaffari...</td>\n",
       "      <td>his</td>\n",
       "      <td>316</td>\n",
       "      <td>Mughal</td>\n",
       "      <td>274</td>\n",
       "      <td>False</td>\n",
       "      <td>Shahjahan</td>\n",
       "      <td>287</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Ahmedabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>validation-27</td>\n",
       "      <td>Bahrain's citizenship law allows for the cabin...</td>\n",
       "      <td>he</td>\n",
       "      <td>330</td>\n",
       "      <td>Ali Khamenei</td>\n",
       "      <td>226</td>\n",
       "      <td>False</td>\n",
       "      <td>Sheikh Isa Qassim</td>\n",
       "      <td>374</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Bahrain%E2%80%93I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>validation-28</td>\n",
       "      <td>In 2000 he stepped down and focused on the Ros...</td>\n",
       "      <td>his</td>\n",
       "      <td>304</td>\n",
       "      <td>Lord Hodgson</td>\n",
       "      <td>79</td>\n",
       "      <td>False</td>\n",
       "      <td>Wade</td>\n",
       "      <td>316</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Michael_Wade_(Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>validation-29</td>\n",
       "      <td>During a rally of Haqqani, Carrie forces Quinn...</td>\n",
       "      <td>him</td>\n",
       "      <td>342</td>\n",
       "      <td>Saul</td>\n",
       "      <td>285</td>\n",
       "      <td>False</td>\n",
       "      <td>Haqqani</td>\n",
       "      <td>325</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Homeland_(TV_series)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>validation-30</td>\n",
       "      <td>In the Prius, unlike some other cars, holding ...</td>\n",
       "      <td>he</td>\n",
       "      <td>309</td>\n",
       "      <td>Voelcker</td>\n",
       "      <td>123</td>\n",
       "      <td>False</td>\n",
       "      <td>Wozniak</td>\n",
       "      <td>142</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2009%E2%80%9311_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>validation-425</td>\n",
       "      <td>Ericsson had a distinguished career in naval d...</td>\n",
       "      <td>his</td>\n",
       "      <td>320</td>\n",
       "      <td>John C</td>\n",
       "      <td>280</td>\n",
       "      <td>False</td>\n",
       "      <td>Gilmer</td>\n",
       "      <td>492</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/USS_Princeton_(1843)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>validation-426</td>\n",
       "      <td>Peter's parish church, Hanwell. George Harris'...</td>\n",
       "      <td>his</td>\n",
       "      <td>246</td>\n",
       "      <td>George</td>\n",
       "      <td>185</td>\n",
       "      <td>False</td>\n",
       "      <td>Nicholas Harris</td>\n",
       "      <td>204</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Fritwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>validation-427</td>\n",
       "      <td>Danielle later breaks down and tells Ronnie th...</td>\n",
       "      <td>he</td>\n",
       "      <td>258</td>\n",
       "      <td>Ronnie</td>\n",
       "      <td>198</td>\n",
       "      <td>False</td>\n",
       "      <td>Andy</td>\n",
       "      <td>236</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Danielle_Jones_(E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>validation-428</td>\n",
       "      <td>On March 23, 1933, middle-aged cigar store own...</td>\n",
       "      <td>her</td>\n",
       "      <td>322</td>\n",
       "      <td>Mary</td>\n",
       "      <td>369</td>\n",
       "      <td>True</td>\n",
       "      <td>Elvina</td>\n",
       "      <td>414</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Turn_Back_the_Clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>validation-429</td>\n",
       "      <td>As a practicing attorney, Ms. Neal specialized...</td>\n",
       "      <td>she</td>\n",
       "      <td>442</td>\n",
       "      <td>Neal</td>\n",
       "      <td>310</td>\n",
       "      <td>True</td>\n",
       "      <td>Lynne V. Cheney</td>\n",
       "      <td>419</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Anne_D._Neal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>validation-430</td>\n",
       "      <td>Ethel Tweedie was born 1 January 1862 in Londo...</td>\n",
       "      <td>Her</td>\n",
       "      <td>165</td>\n",
       "      <td>Ethel Tweedie</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Emma Jessier</td>\n",
       "      <td>90</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Ethel_Brilliana_T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>validation-431</td>\n",
       "      <td>Anam goes to meet Zeeshan in jail where he ask...</td>\n",
       "      <td>his</td>\n",
       "      <td>351</td>\n",
       "      <td>Ansar</td>\n",
       "      <td>285</td>\n",
       "      <td>False</td>\n",
       "      <td>Kamaal</td>\n",
       "      <td>382</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kahi_Unkahi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>validation-432</td>\n",
       "      <td>Deb knows everything about Bani and transforms...</td>\n",
       "      <td>she</td>\n",
       "      <td>334</td>\n",
       "      <td>Deb</td>\n",
       "      <td>171</td>\n",
       "      <td>False</td>\n",
       "      <td>Pallavi</td>\n",
       "      <td>248</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Kasamh_Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>validation-433</td>\n",
       "      <td>He smokes too much, lectures every once in a w...</td>\n",
       "      <td>her</td>\n",
       "      <td>399</td>\n",
       "      <td>Catherine</td>\n",
       "      <td>297</td>\n",
       "      <td>False</td>\n",
       "      <td>Wanda Woman</td>\n",
       "      <td>410</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Gad_Guard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>validation-434</td>\n",
       "      <td>Berberian's death was due to a heart attack, a...</td>\n",
       "      <td>her</td>\n",
       "      <td>303</td>\n",
       "      <td>Cristina</td>\n",
       "      <td>316</td>\n",
       "      <td>False</td>\n",
       "      <td>Berberian</td>\n",
       "      <td>435</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Cathy_Berberian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>validation-435</td>\n",
       "      <td>Finding that his relationship with Natalie has...</td>\n",
       "      <td>her</td>\n",
       "      <td>288</td>\n",
       "      <td>Mia</td>\n",
       "      <td>316</td>\n",
       "      <td>False</td>\n",
       "      <td>Natalie</td>\n",
       "      <td>342</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Love_Actually</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>validation-436</td>\n",
       "      <td>Caroline Spelman, the then Chairman of the Con...</td>\n",
       "      <td>her</td>\n",
       "      <td>374</td>\n",
       "      <td>Spelman</td>\n",
       "      <td>284</td>\n",
       "      <td>True</td>\n",
       "      <td>Tina Haynes</td>\n",
       "      <td>352</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_expenses_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>validation-437</td>\n",
       "      <td>She returned to competition at the 2014 Four C...</td>\n",
       "      <td>she</td>\n",
       "      <td>302</td>\n",
       "      <td>Kanako Murakami</td>\n",
       "      <td>174</td>\n",
       "      <td>False</td>\n",
       "      <td>Satoko Miyahara</td>\n",
       "      <td>217</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Li_Zijun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>validation-438</td>\n",
       "      <td>To reach the quarter final, she had defeated A...</td>\n",
       "      <td>her</td>\n",
       "      <td>457</td>\n",
       "      <td>Rodionova</td>\n",
       "      <td>371</td>\n",
       "      <td>False</td>\n",
       "      <td>Lisicki</td>\n",
       "      <td>396</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Sabine_Lisicki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>validation-439</td>\n",
       "      <td>The two ``Suppositions'' satirise the manners ...</td>\n",
       "      <td>she</td>\n",
       "      <td>603</td>\n",
       "      <td>Miss Coquettilla</td>\n",
       "      <td>191</td>\n",
       "      <td>False</td>\n",
       "      <td>Hands</td>\n",
       "      <td>308</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Elizabeth_Hands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>validation-440</td>\n",
       "      <td>An office pool is started on when Dan will die...</td>\n",
       "      <td>him</td>\n",
       "      <td>281</td>\n",
       "      <td>Jack</td>\n",
       "      <td>210</td>\n",
       "      <td>True</td>\n",
       "      <td>Julius</td>\n",
       "      <td>265</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/The_Good_Guys_(20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>validation-441</td>\n",
       "      <td>In 1959, Vivian Bell, a 35-year-old English pr...</td>\n",
       "      <td>she</td>\n",
       "      <td>440</td>\n",
       "      <td>Cay Rivvers</td>\n",
       "      <td>368</td>\n",
       "      <td>True</td>\n",
       "      <td>Frances</td>\n",
       "      <td>420</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Desert_Hearts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>validation-442</td>\n",
       "      <td>After her sophomore year in college, Lofgren e...</td>\n",
       "      <td>her</td>\n",
       "      <td>6</td>\n",
       "      <td>Lofgren</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>Erin Cafaro</td>\n",
       "      <td>218</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Esther_Lofgren</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>validation-443</td>\n",
       "      <td>Later, a desperate Joe asks Graeme if he could...</td>\n",
       "      <td>he</td>\n",
       "      <td>332</td>\n",
       "      <td>David</td>\n",
       "      <td>311</td>\n",
       "      <td>True</td>\n",
       "      <td>Joe</td>\n",
       "      <td>328</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Joe_McIntyre_(Cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>validation-444</td>\n",
       "      <td>He works from his studio is located in the Tan...</td>\n",
       "      <td>She</td>\n",
       "      <td>647</td>\n",
       "      <td>Bellamacina</td>\n",
       "      <td>776</td>\n",
       "      <td>True</td>\n",
       "      <td>Lorca</td>\n",
       "      <td>828</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Robert_Montgomery...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>validation-445</td>\n",
       "      <td>Henry Orlac is murdered, and Stephen receives ...</td>\n",
       "      <td>his</td>\n",
       "      <td>299</td>\n",
       "      <td>Rollo</td>\n",
       "      <td>258</td>\n",
       "      <td>True</td>\n",
       "      <td>Stephen</td>\n",
       "      <td>278</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Mad_Love_(1935_film)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>validation-446</td>\n",
       "      <td>In order to pay for her lessons with Lorraine,...</td>\n",
       "      <td>her</td>\n",
       "      <td>379</td>\n",
       "      <td>Rankin</td>\n",
       "      <td>294</td>\n",
       "      <td>True</td>\n",
       "      <td>Traubel</td>\n",
       "      <td>334</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Nell_Rankin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>validation-447</td>\n",
       "      <td>The argument ends with Abe's leaving in a huff...</td>\n",
       "      <td>her</td>\n",
       "      <td>259</td>\n",
       "      <td>Megan</td>\n",
       "      <td>64</td>\n",
       "      <td>False</td>\n",
       "      <td>Peggy</td>\n",
       "      <td>146</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Far_Away_Places_(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>validation-448</td>\n",
       "      <td>Francisco: Punisher felt that he was the bigge...</td>\n",
       "      <td>he</td>\n",
       "      <td>342</td>\n",
       "      <td>Francisco</td>\n",
       "      <td>275</td>\n",
       "      <td>False</td>\n",
       "      <td>Craig</td>\n",
       "      <td>301</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/I_Love_Money_(sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>validation-449</td>\n",
       "      <td>The iron-hulled steamer Gertrude was built in ...</td>\n",
       "      <td>her</td>\n",
       "      <td>226</td>\n",
       "      <td>Gertrude</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>Emma</td>\n",
       "      <td>241</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/USS_Gertrude_(1863)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>validation-450</td>\n",
       "      <td>He then agrees to name the gargoyle Goldie, af...</td>\n",
       "      <td>He</td>\n",
       "      <td>305</td>\n",
       "      <td>Lucien</td>\n",
       "      <td>252</td>\n",
       "      <td>False</td>\n",
       "      <td>Abel</td>\n",
       "      <td>264</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Goldie_(DC_Comics)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>validation-451</td>\n",
       "      <td>Disgusted with the family's ``mendacity'', Bri...</td>\n",
       "      <td>she</td>\n",
       "      <td>365</td>\n",
       "      <td>Maggie</td>\n",
       "      <td>242</td>\n",
       "      <td>False</td>\n",
       "      <td>Mae</td>\n",
       "      <td>257</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Cat_on_a_Hot_Tin_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>validation-452</td>\n",
       "      <td>She manipulates Michael into giving her custod...</td>\n",
       "      <td>she</td>\n",
       "      <td>306</td>\n",
       "      <td>Scarlett</td>\n",
       "      <td>255</td>\n",
       "      <td>False</td>\n",
       "      <td>Alice</td>\n",
       "      <td>291</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Michael_Moon_(Eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>validation-453</td>\n",
       "      <td>On April 4, 1986, Donal Henahan wrote in the N...</td>\n",
       "      <td>her</td>\n",
       "      <td>330</td>\n",
       "      <td>Aida</td>\n",
       "      <td>250</td>\n",
       "      <td>False</td>\n",
       "      <td>Miss Millo</td>\n",
       "      <td>294</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Aprile_Millo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>validation-454</td>\n",
       "      <td>Pleasant explains Vassey's guilty conscience m...</td>\n",
       "      <td>him</td>\n",
       "      <td>282</td>\n",
       "      <td>Vassey</td>\n",
       "      <td>234</td>\n",
       "      <td>True</td>\n",
       "      <td>Denton</td>\n",
       "      <td>255</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Small_Crimes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>454 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                        ...                                                                        URL\n",
       "0      validation-1                        ...                          http://en.wikipedia.org/wiki/Commission_on_Ele...\n",
       "1      validation-2                        ...                                 http://en.wikipedia.org/wiki/Kathleen_Nott\n",
       "2      validation-3                        ...                          http://en.wikipedia.org/wiki/Hawaii_Five-0_(20...\n",
       "3      validation-4                        ...                               http://en.wikipedia.org/wiki/Craig_Reucassel\n",
       "4      validation-5                        ...                            http://en.wikipedia.org/wiki/Denys_Finch_Hatton\n",
       "5      validation-6                        ...                          http://en.wikipedia.org/wiki/True-believer_syn...\n",
       "6      validation-7                        ...                                    http://en.wikipedia.org/wiki/Faik_Pasha\n",
       "7      validation-8                        ...                              http://en.wikipedia.org/wiki/Colin_McClelland\n",
       "8      validation-9                        ...                                http://en.wikipedia.org/wiki/Shirley_Temple\n",
       "9     validation-10                        ...                                  http://en.wikipedia.org/wiki/Liz_McDonald\n",
       "10    validation-11                        ...                           http://en.wikipedia.org/wiki/Dimitrios_Kallergis\n",
       "11    validation-12                        ...                                 http://en.wikipedia.org/wiki/Brodie_family\n",
       "12    validation-13                        ...                          http://en.wikipedia.org/wiki/Futari_wa_Pretty_...\n",
       "13    validation-14                        ...                          http://en.wikipedia.org/wiki/It's_Always_Fair_...\n",
       "14    validation-15                        ...                          http://en.wikipedia.org/wiki/Augustus_Moore_He...\n",
       "15    validation-16                        ...                            http://en.wikipedia.org/wiki/Frank_Lloyd_Wright\n",
       "16    validation-17                        ...                          http://en.wikipedia.org/wiki/Soundscapes_by_Ro...\n",
       "17    validation-18                        ...                          http://en.wikipedia.org/wiki/The_State_and_Rev...\n",
       "18    validation-19                        ...                                  http://en.wikipedia.org/wiki/Andy_Bernard\n",
       "19    validation-20                        ...                          http://en.wikipedia.org/wiki/Green_Springs_Nat...\n",
       "20    validation-21                        ...                               http://en.wikipedia.org/wiki/Light_in_August\n",
       "21    validation-22                        ...                          http://en.wikipedia.org/wiki/The_Last_of_Us:_A...\n",
       "22    validation-23                        ...                                 http://en.wikipedia.org/wiki/Carlo_Bonavia\n",
       "23    validation-24                        ...                          http://en.wikipedia.org/wiki/I_Love_Money_(sea...\n",
       "24    validation-25                        ...                               http://en.wikipedia.org/wiki/James_A._Adkins\n",
       "25    validation-26                        ...                                     http://en.wikipedia.org/wiki/Ahmedabad\n",
       "26    validation-27                        ...                          http://en.wikipedia.org/wiki/Bahrain%E2%80%93I...\n",
       "27    validation-28                        ...                          http://en.wikipedia.org/wiki/Michael_Wade_(Tra...\n",
       "28    validation-29                        ...                          http://en.wikipedia.org/wiki/Homeland_(TV_series)\n",
       "29    validation-30                        ...                          http://en.wikipedia.org/wiki/2009%E2%80%9311_T...\n",
       "..              ...                        ...                                                                        ...\n",
       "424  validation-425                        ...                          http://en.wikipedia.org/wiki/USS_Princeton_(1843)\n",
       "425  validation-426                        ...                                      http://en.wikipedia.org/wiki/Fritwell\n",
       "426  validation-427                        ...                          http://en.wikipedia.org/wiki/Danielle_Jones_(E...\n",
       "427  validation-428                        ...                          http://en.wikipedia.org/wiki/Turn_Back_the_Clo...\n",
       "428  validation-429                        ...                                  http://en.wikipedia.org/wiki/Anne_D._Neal\n",
       "429  validation-430                        ...                          http://en.wikipedia.org/wiki/Ethel_Brilliana_T...\n",
       "430  validation-431                        ...                                   http://en.wikipedia.org/wiki/Kahi_Unkahi\n",
       "431  validation-432                        ...                                     http://en.wikipedia.org/wiki/Kasamh_Se\n",
       "432  validation-433                        ...                          http://en.wikipedia.org/wiki/List_of_Gad_Guard...\n",
       "433  validation-434                        ...                               http://en.wikipedia.org/wiki/Cathy_Berberian\n",
       "434  validation-435                        ...                                 http://en.wikipedia.org/wiki/Love_Actually\n",
       "435  validation-436                        ...                          http://en.wikipedia.org/wiki/List_of_expenses_...\n",
       "436  validation-437                        ...                                      http://en.wikipedia.org/wiki/Li_Zijun\n",
       "437  validation-438                        ...                                http://en.wikipedia.org/wiki/Sabine_Lisicki\n",
       "438  validation-439                        ...                               http://en.wikipedia.org/wiki/Elizabeth_Hands\n",
       "439  validation-440                        ...                          http://en.wikipedia.org/wiki/The_Good_Guys_(20...\n",
       "440  validation-441                        ...                                 http://en.wikipedia.org/wiki/Desert_Hearts\n",
       "441  validation-442                        ...                                http://en.wikipedia.org/wiki/Esther_Lofgren\n",
       "442  validation-443                        ...                          http://en.wikipedia.org/wiki/Joe_McIntyre_(Cor...\n",
       "443  validation-444                        ...                          http://en.wikipedia.org/wiki/Robert_Montgomery...\n",
       "444  validation-445                        ...                          http://en.wikipedia.org/wiki/Mad_Love_(1935_film)\n",
       "445  validation-446                        ...                                   http://en.wikipedia.org/wiki/Nell_Rankin\n",
       "446  validation-447                        ...                          http://en.wikipedia.org/wiki/Far_Away_Places_(...\n",
       "447  validation-448                        ...                          http://en.wikipedia.org/wiki/I_Love_Money_(sea...\n",
       "448  validation-449                        ...                           http://en.wikipedia.org/wiki/USS_Gertrude_(1863)\n",
       "449  validation-450                        ...                            http://en.wikipedia.org/wiki/Goldie_(DC_Comics)\n",
       "450  validation-451                        ...                          http://en.wikipedia.org/wiki/Cat_on_a_Hot_Tin_...\n",
       "451  validation-452                        ...                          http://en.wikipedia.org/wiki/Michael_Moon_(Eas...\n",
       "452  validation-453                        ...                                  http://en.wikipedia.org/wiki/Aprile_Millo\n",
       "453  validation-454                        ...                                  http://en.wikipedia.org/wiki/Small_Crimes\n",
       "\n",
       "[454 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import DependencyParser\n",
    "import spacy\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "def bs(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid + 1\n",
    "    return lo\n",
    "\n",
    "def _get_preceding_words(tokens, offset, k):\n",
    "    start = offset - k\n",
    "    \n",
    "    precedings = [None] * max(0, 0-start)\n",
    "    start = max(0, start)\n",
    "    precedings += tokens[start: offset]\n",
    "    \n",
    "    return precedings\n",
    "\n",
    "def _get_following_words(tokens, offset, k):\n",
    "    end = offset + k\n",
    "    \n",
    "    followings = [None] * max(0, end - len(tokens))\n",
    "    end = min(len(tokens), end)\n",
    "    followings += tokens[offset: end]\n",
    "    \n",
    "    return followings\n",
    "        \n",
    "\n",
    "def extrac_embed_features_tokens(text, char_offset):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # char offset to token offset\n",
    "    lens = [token.idx for token in doc]\n",
    "    mention_offset = bs(lens, char_offset) - 1\n",
    "    # mention_word\n",
    "    mention = doc[mention_offset]\n",
    "    \n",
    "    # token offset to sentence offset\n",
    "    lens = [len(sent) for sent in doc.sents]\n",
    "    acc_lens = [len_ for len_ in lens]\n",
    "    pre_len = 0\n",
    "    for i in range(0, len(acc_lens)):\n",
    "        pre_len += acc_lens[i]\n",
    "        acc_lens[i] = pre_len\n",
    "    sent_index = bs(acc_lens, mention_offset)\n",
    "    # mention sentence\n",
    "    sent = list(doc.sents)[sent_index]\n",
    "    \n",
    "    # dependency parent\n",
    "    head = mention.head\n",
    "    \n",
    "    # last word and first word\n",
    "    first_word, last_word = sent[0], sent[-2]\n",
    "    \n",
    "    assert mention_offset >= 0\n",
    "    \n",
    "    # two preceding words and two following words\n",
    "    tokens = list(doc)\n",
    "    precedings2 = _get_preceding_words(tokens, mention_offset, 2)\n",
    "    followings2 = _get_following_words(tokens, mention_offset, 2)\n",
    "    \n",
    "    # five preceding words and five following words\n",
    "    precedings5 = _get_preceding_words(tokens, mention_offset, 5)\n",
    "    followings5 = _get_following_words(tokens, mention_offset, 5)\n",
    "    \n",
    "    # sentence words\n",
    "    sent_tokens = [token for token in sent]\n",
    "    \n",
    "    return mention, head, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: \n",
      "Abhisek played a role in the play.His performance was quite good\n",
      "\n",
      "Dependency parsing trees: \n",
      "        played          \n",
      "    ______|__________    \n",
      "   |      |     |    in \n",
      "   |      |     |    |   \n",
      "   |      |    role play\n",
      "   |      |     |    |   \n",
      "Abhisek   .     a   the \n",
      "\n",
      "            was      \n",
      "      _______|____    \n",
      "performance      good\n",
      "     |            |   \n",
      "    His         quite\n",
      "\n",
      "\n",
      "Features:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mention                                       quite\n",
       "parent                                         good\n",
       "first_word                                      His\n",
       "last_word                                     quite\n",
       "precedings2                      [performance, was]\n",
       "followings2                           [quite, good]\n",
       "precedings5        [play, ., His, performance, was]\n",
       "followings5         [None, None, None, quite, good]\n",
       "sent_tokens    [His, performance, was, quite, good]\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Texts: \")\n",
    "text = u\"Abhisek played a role in the play.His performance was quite good\"\n",
    "print(text)\n",
    "\n",
    "print(\"\\nDependency parsing trees: \")\n",
    "doc = nlp(text)\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]\n",
    "\n",
    "print(\"\\nFeatures:\")\n",
    "mention, parent, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens = extrac_embed_features_tokens(text, 274)\n",
    "features = pd.Series([str(feature) for feature in (mention, parent, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens)], index=['mention', 'parent', 'first_word', 'last_word', 'precedings2', 'followings2', 'precedings5', 'followings5', 'sent_tokens'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embed_features = 11\n",
    "embed_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_features(df, text_column, offset_column):\n",
    "    text_offset_list = df[[text_column, offset_column]].values.tolist()\n",
    "    num_features = num_embed_features\n",
    "    \n",
    "    embed_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features, embed_dim))\n",
    "    for text_offset_index in range(len(text_offset_list)):\n",
    "        text_offset = text_offset_list[text_offset_index]\n",
    "        mention, parent, first_word, last_word, precedings2, followings2, precedings5, followings5, sent_tokens = extrac_embed_features_tokens(text_offset[0], text_offset[1])\n",
    "        \n",
    "        feature_index = 0\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = mention.vector\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = parent.vector\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = first_word.vector\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = last_word.vector\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index:feature_index+2, :] = np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in precedings2])\n",
    "        feature_index += len(precedings2)\n",
    "        embed_feature_matrix[text_offset_index, feature_index:feature_index+2, :] = np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in followings2])\n",
    "        feature_index += len(followings2)\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = np.mean(np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in precedings5]), axis=0)\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = np.mean(np.asarray([token.vector if token is not None else np.zeros((embed_dim,)) for token in followings5]), axis=0)\n",
    "        feature_index += 1\n",
    "        embed_feature_matrix[text_offset_index, feature_index, :] = np.mean(np.asarray([token.vector for token in sent_tokens]), axis=0) if len(sent_tokens) > 0 else np.zeros(embed_dim)\n",
    "        feature_index += 1\n",
    "    \n",
    "    return embed_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs_(list_, target_):\n",
    "    lo, hi = 0, len(list_) -1\n",
    "    \n",
    "    while lo < hi:\n",
    "        mid = lo + int((hi - lo) / 2)\n",
    "        \n",
    "        if target_ < list_[mid]:\n",
    "            hi = mid\n",
    "        elif target_ > list_[mid]:\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            return mid\n",
    "    return lo\n",
    "\n",
    "def ohe_dist(dist, buckets):\n",
    "    idx = bs_(buckets, dist)\n",
    "    oh = np.zeros(shape=(len(buckets),), dtype=np.float32)\n",
    "    oh[idx] = 1\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrac_positional_features(text, char_offset1, char_offset2):\n",
    "    doc = nlp(text)\n",
    "    max_len = 64\n",
    "    \n",
    "    # char offset to token offset\n",
    "    lens = [token.idx for token in doc]\n",
    "    mention_offset1 = bs(lens, char_offset1) - 1\n",
    "    mention_offset2 = bs(lens, char_offset2) - 1\n",
    "    \n",
    "    # token offset to sentence offset\n",
    "    lens = [len(sent) for sent in doc.sents]\n",
    "    acc_lens = [len_ for len_ in lens]\n",
    "    pre_len = 0\n",
    "    for i in range(0, len(acc_lens)):\n",
    "        pre_len += acc_lens[i]\n",
    "        acc_lens[i] = pre_len\n",
    "    sent_index1 = bs(acc_lens, mention_offset1)\n",
    "    sent_index2 = bs(acc_lens, mention_offset2)\n",
    "    \n",
    "    sent1 = list(doc.sents)[sent_index1]\n",
    "    sent2 = list(doc.sents)[sent_index2]\n",
    "    \n",
    "    # buckets\n",
    "    bucket_dist = [1, 2, 3, 4, 5, 8, 16, 32, 64]\n",
    "    \n",
    "    # relative distance\n",
    "    dist = mention_offset2 - mention_offset1\n",
    "    dist_oh = ohe_dist(dist, bucket_dist)\n",
    "    \n",
    "    # buckets\n",
    "    bucket_pos = [0, 1, 2, 3, 4, 5, 8, 16, 32]\n",
    "    \n",
    "    # absolute position in the sentence\n",
    "    sent_pos1 = mention_offset1 + 1\n",
    "    if sent_index1 > 0:\n",
    "        sent_pos1 = mention_offset1 - acc_lens[sent_index1-1]\n",
    "    sent_pos_oh1 = ohe_dist(sent_pos1, bucket_pos)\n",
    "    sent_pos_inv1 = len(sent1) - sent_pos1\n",
    "    assert sent_pos_inv1 >= 0\n",
    "    sent_pos_inv_oh1 = ohe_dist(sent_pos_inv1, bucket_pos)\n",
    "    \n",
    "    sent_pos2 = mention_offset2 + 1\n",
    "    if sent_index2 > 0:\n",
    "        sent_pos2 = mention_offset2 - acc_lens[sent_index2-1]\n",
    "    sent_pos_oh2 = ohe_dist(sent_pos2, bucket_pos)\n",
    "    sent_pos_inv2 = len(sent2) - sent_pos2\n",
    "    if sent_pos_inv2 < 0:\n",
    "        print(sent_pos_inv2)\n",
    "        print(len(sent2))\n",
    "        print(sent_pos2)\n",
    "        raise ValueError\n",
    "    sent_pos_inv_oh2 = ohe_dist(sent_pos_inv2, bucket_pos)\n",
    "    \n",
    "    sent_pos_ratio1 = sent_pos1 / len(sent1)\n",
    "    sent_pos_ratio2 = sent_pos2 / len(sent2)\n",
    "    \n",
    "    return dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos_features = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_features(df, text_column, pronoun_offset_column, name_offset_column):\n",
    "    text_offset_list = df[[text_column, pronoun_offset_column, name_offset_column]].values.tolist()\n",
    "    num_features = num_pos_features\n",
    "    \n",
    "    pos_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features))\n",
    "    for text_offset_index in range(len(text_offset_list)):\n",
    "        text_offset = text_offset_list[text_offset_index]\n",
    "        dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2 = extrac_positional_features(text_offset[0], text_offset[1], text_offset[2])\n",
    "        \n",
    "        feature_index = 0\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(dist_oh)] = np.asarray(dist_oh)\n",
    "        feature_index += len(dist_oh)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh1)] = np.asarray(sent_pos_oh1)\n",
    "        feature_index += len(sent_pos_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh2)] = np.asarray(sent_pos_oh2)\n",
    "        feature_index += len(sent_pos_oh2)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh1)] = np.asarray(sent_pos_inv_oh1)\n",
    "        feature_index += len(sent_pos_inv_oh1)\n",
    "        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh2)] = np.asarray(sent_pos_inv_oh2)\n",
    "        feature_index += len(sent_pos_inv_oh2)\n",
    "    \n",
    "    return pos_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_emb_tra = create_embedding_features(train_df, 'Text', 'Pronoun-offset')\n",
    "p_emb_dev = create_embedding_features(dev_df, 'Text', 'Pronoun-offset')\n",
    "p_emb_test = create_embedding_features(test_df, 'Text', 'Pronoun-offset')\n",
    "\n",
    "a_emb_tra = create_embedding_features(train_df, 'Text', 'A-offset')\n",
    "a_emb_dev = create_embedding_features(dev_df, 'Text', 'A-offset')\n",
    "a_emb_test = create_embedding_features(test_df, 'Text', 'A-offset')\n",
    "\n",
    "b_emb_tra = create_embedding_features(train_df, 'Text', 'B-offset')\n",
    "b_emb_dev = create_embedding_features(dev_df, 'Text', 'B-offset')\n",
    "b_emb_test = create_embedding_features(test_df, 'Text', 'B-offset')\n",
    "\n",
    "pa_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "pa_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'A-offset')\n",
    "\n",
    "pb_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'B-offset')\n",
    "pb_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'B-offset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test-1</td>\n",
       "      <td>Upon their acceptance into the Kontinental Hoc...</td>\n",
       "      <td>His</td>\n",
       "      <td>383</td>\n",
       "      <td>Bob Suter</td>\n",
       "      <td>352</td>\n",
       "      <td>False</td>\n",
       "      <td>Dehner</td>\n",
       "      <td>366</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jeremy_Dehner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test-2</td>\n",
       "      <td>Between the years 1979-1981, River won four lo...</td>\n",
       "      <td>him</td>\n",
       "      <td>430</td>\n",
       "      <td>Alonso</td>\n",
       "      <td>353</td>\n",
       "      <td>True</td>\n",
       "      <td>Alfredo Di St*fano</td>\n",
       "      <td>390</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Norberto_Alonso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test-3</td>\n",
       "      <td>Though his emigration from the country has aff...</td>\n",
       "      <td>He</td>\n",
       "      <td>312</td>\n",
       "      <td>Ali Aladhadh</td>\n",
       "      <td>256</td>\n",
       "      <td>True</td>\n",
       "      <td>Saddam</td>\n",
       "      <td>295</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Aladhadh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test-4</td>\n",
       "      <td>At the trial, Pisciotta said: ``Those who have...</td>\n",
       "      <td>his</td>\n",
       "      <td>526</td>\n",
       "      <td>Alliata</td>\n",
       "      <td>377</td>\n",
       "      <td>False</td>\n",
       "      <td>Pisciotta</td>\n",
       "      <td>536</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Gaspare_Pisciotta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test-5</td>\n",
       "      <td>It is about a pair of United States Navy shore...</td>\n",
       "      <td>his</td>\n",
       "      <td>406</td>\n",
       "      <td>Eddie</td>\n",
       "      <td>421</td>\n",
       "      <td>True</td>\n",
       "      <td>Rock Reilly</td>\n",
       "      <td>559</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Chasers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID                       ...                                                                   URL\n",
       "0  test-1                       ...                            http://en.wikipedia.org/wiki/Jeremy_Dehner\n",
       "1  test-2                       ...                          http://en.wikipedia.org/wiki/Norberto_Alonso\n",
       "2  test-3                       ...                                 http://en.wikipedia.org/wiki/Aladhadh\n",
       "3  test-4                       ...                        http://en.wikipedia.org/wiki/Gaspare_Pisciotta\n",
       "4  test-5                       ...                                  http://en.wikipedia.org/wiki/Chasers\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _row_to_y(row):\n",
    "    if row.loc['A-coref']:\n",
    "        return 0\n",
    "    if row.loc['B-coref']:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "y_tra = train_df.apply(_row_to_y, axis=1)\n",
    "y_dev = dev_df.apply(_row_to_y, axis=1)\n",
    "y_test = test_df.apply(_row_to_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [p_emb_tra, a_emb_tra, b_emb_tra, pa_pos_tra, pb_pos_tra]\n",
    "X_dev = [p_emb_dev, a_emb_dev, b_emb_dev, pa_pos_dev, pb_pos_dev]\n",
    "X_test = [p_emb_test, a_emb_test, b_emb_test, pa_pos_test, pb_pos_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import backend\n",
    "from keras import layers\n",
    "from keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    model_dim, mlp_dim, mlp_depth=1, drop_out=0.5, return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A Multi-Layer Perceptron Model.\n",
    "    \n",
    "    inputs: \n",
    "        embeddings: [batch, num_embed_feature, embed_dims] * 3 ## pronoun, A, B\n",
    "        positional_features: [batch, num_pos_feature] * 2 ## pronoun-A, pronoun-B\n",
    "        \n",
    "    outputs: \n",
    "        [batch, num_classes] # in our case there should be 3 output classes: A, B, None\n",
    "        \n",
    "    :param output_dim: the output dimension size\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    def _mlp_channel1(feature_dropout_layer, feature_map_layer, flatten_layer, x):\n",
    "        x = feature_dropout_layer(x)\n",
    "        x = feature_map_layer(x)\n",
    "        x = flatten_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _mlp_channel2(feature_map_layer, x):\n",
    "        x = feature_map_layer(x)\n",
    "        return x\n",
    "\n",
    "    # inputs\n",
    "    inputs1 = list()\n",
    "    for fi in range(num_feature_channels1):\n",
    "        inputs1.append(models.Input(shape=(num_features1, feature_dim1), dtype='float32', name='input1_' + str(fi)))\n",
    "        \n",
    "    inputs2 = list()\n",
    "    for fi in range(num_feature_channels2):\n",
    "        inputs2.append(models.Input(shape=(num_features2, ), dtype='float32', name='input2_' + str(fi)))\n",
    "    \n",
    "    # define feature map layers\n",
    "    # MLP Layers\n",
    "    feature_dropout_layer1 = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"input_dropout_layer\"))\n",
    "    feature_map_layer1 = layers.TimeDistributed(layers.Dense(model_dim, name=\"feature_map_layer1\", activation=\"relu\"))\n",
    "    flatten_layer1 = layers.Flatten(name=\"feature_flatten_layer1\")\n",
    "    feature_map_layer2 = layers.Dense(model_dim, name=\"feature_map_layer2\", activation=\"relu\")\n",
    "    \n",
    "    x1 = [_mlp_channel1(feature_dropout_layer1, feature_map_layer1, flatten_layer1, input_) for input_ in inputs1]\n",
    "    x2 = [_mlp_channel2(feature_map_layer2, input_) for input_ in inputs2]\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concate_layer\")(x1+x2)\n",
    "    \n",
    "    # MLP Layers\n",
    "    x = layers.BatchNormalization(name='batch_norm_layer')(x)\n",
    "    x = layers.Dropout(rate=drop_out, name=\"dropout_layer\")(x)\n",
    "        \n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model(inputs1 + inputs2, outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multi_channel_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, model_dim, mlp_dim, \n",
    "    mlp_depth=1, drop_out=0.5, pooling='max', padding='valid', return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A Multi-Layer Perceptron Model.\n",
    "    \n",
    "    inputs: \n",
    "        embeddings: [batch, num_embed_feature, embed_dims] * 3 ## pronoun, A, B\n",
    "        positional_features: [batch, num_pos_feature] * 2 ## pronoun-A, pronoun-B\n",
    "        \n",
    "    outputs: \n",
    "        [batch, num_classes] # in our case there should be 3 output classes: A, B, None\n",
    "        \n",
    "    :param output_dim: the output dimension size\n",
    "    :param num_filters: list of integers\n",
    "        The number of filters.\n",
    "    :param filter_sizes: list of integers\n",
    "        The kernel size.\n",
    "    :param pooling: str, either 'max' or 'average'\n",
    "        Pooling method.\n",
    "    :param padding: One of \"valid\", \"causal\" or \"same\" (case-insensitive).\n",
    "        Padding method.\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    def _mlp_channel1(feature_dropout_layer, cnns, pools, concate_layer1, x):\n",
    "        x = feature_dropout_layer(x)\n",
    "        pooled_outputs = []\n",
    "        for i in range(len(cnns)):\n",
    "            conv = cnns[i](x)\n",
    "            if pooling == 'max':\n",
    "                conv = pools[i](conv)\n",
    "            else:\n",
    "                conv = pools[i](conv)\n",
    "            pooled_outputs.append(conv)\n",
    "        \n",
    "        if len(cnns) == 1:\n",
    "            x = conv\n",
    "        else:\n",
    "            x = concate_layer1(pooled_outputs)\n",
    "        return x\n",
    "    \n",
    "    def _mlp_channel2(feature_map_layer, x):\n",
    "        x = feature_map_layer(x)\n",
    "        return x\n",
    "\n",
    "    # inputs\n",
    "    inputs1 = list()\n",
    "    for fi in range(num_feature_channels1):\n",
    "        inputs1.append(models.Input(shape=(num_features1, feature_dim1), dtype='float32', name='input1_' + str(fi)))\n",
    "        \n",
    "    inputs2 = list()\n",
    "    for fi in range(num_feature_channels2):\n",
    "        inputs2.append(models.Input(shape=(num_features2, ), dtype='float32', name='input2_' + str(fi)))\n",
    "    \n",
    "    # define feature map layers\n",
    "    # CNN Layers\n",
    "    cnns = []\n",
    "    pools = []\n",
    "    feature_dropout_layer1 = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"input_dropout_layer\"))\n",
    "    for i in range(len(filter_sizes)):\n",
    "        cnns.append(layers.Conv1D(num_filters[i], kernel_size=filter_sizes[i], padding=padding, activation='relu', name=\"cc_layer1\" + str(i)))\n",
    "        if pooling == 'max':\n",
    "            pools.append(layers.GlobalMaxPooling1D(name='global_pooling_layer1' + str(i)))\n",
    "        else:\n",
    "            pools.append(layers.GlobalAveragePooling1D(name='global_pooling_layer1' + str(i)))\n",
    "    concate_layer1 = layers.Concatenate(name='concated_layer')\n",
    "    \n",
    "    feature_map_layer2 = layers.Dense(model_dim, name=\"feature_map_layer2\", activation=\"relu\")\n",
    "    \n",
    "    x1 = [_mlp_channel1(feature_dropout_layer1, cnns, pools, concate_layer1, input_) for input_ in inputs1]\n",
    "    x2 = [_mlp_channel2(feature_map_layer2, input_) for input_ in inputs2]\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concate_layer\")(x1+x2)\n",
    "    \n",
    "    # MLP Layers\n",
    "    x = layers.BatchNormalization(name='batch_norm_layer')(x)\n",
    "    x = layers.Dropout(rate=drop_out, name=\"dropout_layer\")(x)\n",
    "        \n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model(inputs1 + inputs2, outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import initializers, regularizers, constraints, activations\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "class RemappedCoAttentionWeight(merge._Merge):\n",
    "    \"\"\"\n",
    "        Unnormalized Co-Attention operation for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Ankur et al. [https://aclweb.org/anthology/D16-1244]\n",
    "        \"A Decomposable Attention Model for Natural Language Inference\"\n",
    "        # Input shape\n",
    "            List of 2 3D tensor with shape: `(samples, steps1, features1)` and `(samples, steps2, features2)`.\n",
    "        # Output shape\n",
    "            3D tensor with shape: `(samples, steps1, step2)`.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, model_size, activation='sigmoid',\n",
    "                 W1_regularizer=None,  b1_regularizer=None,\n",
    "                 W1_constraint=None, b1_constraint=None,\n",
    "                 bias1=True, **kwargs):\n",
    "\n",
    "        self.model_size = model_size\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W1_regularizer = regularizers.get(W1_regularizer)\n",
    "        self.b1_regularizer = regularizers.get(b1_regularizer)\n",
    "\n",
    "        self.W1_constraint = constraints.get(W1_constraint)\n",
    "        self.b1_constraint = constraints.get(b1_constraint)\n",
    "\n",
    "        self.bias1 = bias1\n",
    "        self.activation = activations.get(activation)\n",
    "        super(RemappedCoAttentionWeight, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(RemappedCoAttentionWeight, self).build(input_shape)\n",
    "        if len(input_shape) != 2:\n",
    "            raise ValueError(\"input must be a size two list which contains two tensors\")\n",
    "\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "\n",
    "        self.W1 = self.add_weight((self.model_size, shape1[-1]),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W1'.format(self.name),\n",
    "                                 regularizer=self.W1_regularizer,\n",
    "                                 constraint=self.W1_constraint)\n",
    "\n",
    "        self.W2 = self.W1\n",
    "\n",
    "        if self.bias1:\n",
    "            self.b1 = self.add_weight((self.model_size,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b1'.format(self.name),\n",
    "                                     regularizer=self.b1_regularizer,\n",
    "                                     constraint=self.b1_constraint)\n",
    "\n",
    "        if self.bias1:\n",
    "            self.b2 = self.b1\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # pass the mask to the next layers\n",
    "        return input_mask\n",
    "\n",
    "    def _merge_function(self, inputs):\n",
    "        if len(inputs) != 2:\n",
    "            raise ValueError('A `Subtract` layer should be called '\n",
    "                             'on exactly 2 inputs')\n",
    "\n",
    "        x1, x2 = inputs[0], inputs[1]\n",
    "\n",
    "        # u = Wx + b\n",
    "        u1 = _dot_product(x1, self.W1)\n",
    "        if self.bias1:\n",
    "            u1 += self.b1\n",
    "\n",
    "        u2 = _dot_product(x2, self.W2)\n",
    "        if self.bias1:\n",
    "            u2 += self.b2\n",
    "\n",
    "        # u = Activation(Wx + b)\n",
    "        u1 = self.activation(u1)\n",
    "        u2 = self.activation(u2)\n",
    "\n",
    "        # atten = exp(u1 u2^T)\n",
    "        atten = K.batch_dot(u1, u2, axes=[2, 2])\n",
    "        atten = K.exp(atten)\n",
    "\n",
    "        return atten\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not isinstance(input_shape, list) or len(input_shape) != 2:\n",
    "            raise ValueError('A `Dot` layer should be called '\n",
    "                             'on a list of 2 inputs.')\n",
    "        shape1 = list(input_shape[0])\n",
    "        shape2 = list(input_shape[1])\n",
    "\n",
    "        if shape1[0] != shape2[0]:\n",
    "            raise ValueError(\"batch size must be same\")\n",
    "\n",
    "        return shape1[0], shape1[1], shape2[1]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'activation': self.activation,\n",
    "            'model_size': self.model_size,\n",
    "            'W1_regularizer': regularizers.serialize(self.W1_regularizer),\n",
    "            'b1_regularizer': regularizers.serialize(self.b1_regularizer),\n",
    "            'W1_constraint': constraints.serialize(self.W1_constraint),\n",
    "            'b1_constraint': constraints.serialize(self.b1_constraint),\n",
    "            'bias1': self.bias1,\n",
    "        }\n",
    "        base_config = super(RemappedCoAttentionWeight, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "class FeatureNormalization(Layer):\n",
    "    \"\"\"\n",
    "        Normalize feature along a specific axis.\n",
    "        Supports Masking.\n",
    "\n",
    "        # Input shape\n",
    "            A ND tensor with shape: `(samples, feature1 ... featuresN).\n",
    "        # Output shape\n",
    "            ND tensor with shape: `(samples, feature1 ... featuresN)`.\n",
    "        :param kwargs:\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "\n",
    "        self.axis = axis\n",
    "        self.supports_masking = True\n",
    "        super(FeatureNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        super(FeatureNormalization, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # don't pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a = K.cast(mask, K.floatx()) * inputs\n",
    "        else:\n",
    "            a = inputs\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=self.axis, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'axis': self.axis\n",
    "        }\n",
    "        base_config = super(FeatureNormalization, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inter_coattention_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, atten_dim, model_dim, mlp_dim, \n",
    "    mlp_depth=1, drop_out=0.5, pooling='max', padding='valid', return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A Multi-Layer Perceptron Model with Coattention Mechanism.\n",
    "    \n",
    "    inputs: \n",
    "        embeddings: [batch, num_embed_feature, embed_dims] * 3 ## pronoun, A, B\n",
    "        positional_features: [batch, num_pos_feature] * 2 ## pronoun-A, pronoun-B\n",
    "        \n",
    "    outputs: \n",
    "        [batch, num_classes] # in our case there should be 3 output classes: A, B, None\n",
    "        \n",
    "    :param output_dim: the output dimension size\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    def _mlp_channel1(feature_dropout_layer, x):\n",
    "        #x = feature_dropout_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _mlp_channel2(feature_map_layer, x):\n",
    "        x = feature_map_layer(x)\n",
    "        return x\n",
    "\n",
    "    # inputs\n",
    "    inputs1 = list()\n",
    "    for fi in range(num_feature_channels1):\n",
    "        inputs1.append(models.Input(shape=(num_features1, feature_dim1), dtype='float32', name='input1_' + str(fi)))\n",
    "        \n",
    "    inputs2 = list()\n",
    "    for fi in range(num_feature_channels2):\n",
    "        inputs2.append(models.Input(shape=(num_features2, ), dtype='float32', name='input2_' + str(fi)))\n",
    "    \n",
    "    # define feature map layers\n",
    "    # MLP Layers\n",
    "    feature_dropout_layer1 = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"input_dropout_layer\"))\n",
    "    feature_map_layer2 = layers.Dense(feature_dim1, name=\"feature_map_layer2\", activation=\"relu\")\n",
    "    \n",
    "    x1 = [_mlp_channel1(feature_dropout_layer1, input_) for input_ in inputs1]\n",
    "    x2 = [_mlp_channel2(feature_map_layer2, input_) for input_ in inputs2]\n",
    "    \n",
    "    # From mention-pair embeddings\n",
    "    reshape_layer = layers.Reshape((1, feature_dim1), name=\"reshape_layer\")\n",
    "    x2 = [reshape_layer(x2_) for x2_ in x2]\n",
    "    pair1 = layers.Concatenate(axis=1, name=\"concate_pair1_layer\")([x1[0], x1[1], x2[0]])\n",
    "    pair2 = layers.Concatenate(axis=1, name=\"concate_pair2_layer\")([x1[0], x1[2], x2[1]])\n",
    "    \n",
    "    coatten_layer = RemappedCoAttentionWeight(atten_dim, name=\"coattention_weights_layer\")\n",
    "    featnorm_layer1 = FeatureNormalization(name=\"normalized_coattention_weights_layer1\", axis=1)\n",
    "    featnorm_layer2 = FeatureNormalization(name=\"normalized_coattention_weights_layer2\", axis=2)\n",
    "    focus_layer1 = layers.Dot((1, 1), name=\"focus_layer1\")\n",
    "    focus_layer2 = layers.Dot((2, 1), name=\"focus_layer2\")\n",
    "    pair_layer1 = layers.Concatenate(axis=-1, name=\"pair_layer1\")\n",
    "    pair_layer2 = layers.Concatenate(axis=-1, name=\"pair_layer2\")\n",
    "    \n",
    "    # attention\n",
    "    attens = coatten_layer([pair1, pair2])\n",
    "    attens1 = featnorm_layer1(attens)\n",
    "    attens2 = featnorm_layer2(attens)\n",
    "    # compare\n",
    "    focus1 = focus_layer1([attens1, pair1])\n",
    "    focus2 = focus_layer2([attens2, pair2])\n",
    "    pair1 = pair_layer1([pair1, focus2])\n",
    "    pair2 = pair_layer2([pair2, focus1])\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concate_layer\")([pair1, pair2])\n",
    "    x = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"pair_dropout_layer\"))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(mlp_dim, name=\"pair_feature_map_layer\", activation=\"relu\"))(x)\n",
    "    x = layers.Flatten(name=\"pair_feature_flatten_layer1\")(x)\n",
    "    \n",
    "#     pooled_outputs = []\n",
    "#     for i in range(len(filter_sizes)):\n",
    "#         conv = layers.Conv1D(num_filters[i], kernel_size=filter_sizes[i], padding=padding, activation='relu')(x)\n",
    "#         if pooling == 'max':\n",
    "#             conv = layers.GlobalMaxPooling1D(name='global_pooling_layer' + str(i))(conv)\n",
    "#         else:\n",
    "#             conv = layers.GlobalAveragePooling1D(name='global_pooling_layer' + str(i))(conv)\n",
    "#         pooled_outputs.append(conv)\n",
    "#     if len(pooled_outputs) > 1:\n",
    "#         x = layers.Concatenate(name='concated_layer')(pooled_outputs)\n",
    "#     else:\n",
    "#         x = conv\n",
    "    \n",
    "    # MLP Layers\n",
    "    x = layers.BatchNormalization(name='batch_norm_layer')(x)\n",
    "    x = layers.Dropout(rate=drop_out, name=\"dropout_layer\")(x)\n",
    "        \n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model(inputs1 + inputs2, outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {'RemappedCoAttentionWeight': RemappedCoAttentionWeight,\n",
    "                       \"FeatureNormalization\": FeatureNormalization}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_intra_coattention_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, atten_dim, model_dim, mlp_dim, \n",
    "    mlp_depth=1, drop_out=0.5, pooling='max', padding='valid', return_customized_layers=False):\n",
    "    \"\"\"\n",
    "    Create A Multi-Layer Perceptron Model with Coattention Mechanism.\n",
    "    \n",
    "    inputs: \n",
    "        embeddings: [batch, num_embed_feature, embed_dims] * 3 ## pronoun, A, B\n",
    "        positional_features: [batch, num_pos_feature] * 2 ## pronoun-A, pronoun-B\n",
    "        \n",
    "    outputs: \n",
    "        [batch, num_classes] # in our case there should be 3 output classes: A, B, None\n",
    "        \n",
    "    :param output_dim: the output dimension size\n",
    "    :param model_dim: rrn dimension size\n",
    "    :param mlp_dim: the dimension size of fully connected layer\n",
    "    :param mlp_depth: the depth of fully connected layers\n",
    "    :param drop_out: dropout rate of fully connected layers\n",
    "    :param return_customized_layers: boolean, default=False\n",
    "        If True, return model and customized object dictionary, otherwise return model only\n",
    "    :return: keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    def _mlp_channel1(feature_dropout_layer, x):\n",
    "        #x = feature_dropout_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def _mlp_channel2(feature_map_layer, x):\n",
    "        x = feature_map_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def coatten_compare(\n",
    "        feature_concat_layer, coatten_layer, \n",
    "        featnorm_layer1, featnorm_layer2, \n",
    "        focus_layer1, focus_layer2, \n",
    "        pair_layer1, pair_layer2, \n",
    "        mention, entity, mention_entity_feature):\n",
    "        \n",
    "        x1 = feature_concat_layer([entity, mention_entity_feature])\n",
    "        x2 = feature_concat_layer([mention, mention_entity_feature])\n",
    "        \n",
    "        # attention\n",
    "        attens = coatten_layer([x1, x2])\n",
    "        attens1 = featnorm_layer1(attens)\n",
    "        attens2 = featnorm_layer2(attens)\n",
    "        # compare\n",
    "        focus1 = focus_layer1([attens1, x1])\n",
    "        focus2 = focus_layer2([attens2, x2])\n",
    "        x1 = pair_layer1([x1, focus2])\n",
    "        x2 = pair_layer2([x2, focus1])\n",
    "        \n",
    "        return x1, x2\n",
    "\n",
    "    # inputs\n",
    "    inputs1 = list()\n",
    "    for fi in range(num_feature_channels1):\n",
    "        inputs1.append(models.Input(shape=(num_features1, feature_dim1), dtype='float32', name='input1_' + str(fi)))\n",
    "        \n",
    "    inputs2 = list()\n",
    "    for fi in range(num_feature_channels2):\n",
    "        inputs2.append(models.Input(shape=(num_features2, ), dtype='float32', name='input2_' + str(fi)))\n",
    "    \n",
    "    # define feature map layers\n",
    "    # MLP Layers\n",
    "    feature_dropout_layer1 = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"input_dropout_layer\"))\n",
    "    feature_map_layer2 = layers.Dense(feature_dim1, name=\"feature_map_layer2\", activation=\"relu\")\n",
    "    \n",
    "    x1 = [_mlp_channel1(feature_dropout_layer1, input_) for input_ in inputs1]\n",
    "    x2 = [_mlp_channel2(feature_map_layer2, input_) for input_ in inputs2]\n",
    "    \n",
    "    # From mention-pair embeddings\n",
    "    reshape_layer = layers.Reshape((1, feature_dim1), name=\"reshape_layer\")\n",
    "    x2 = [reshape_layer(x2_) for x2_ in x2]\n",
    "    \n",
    "    feature_concat_layer = layers.Concatenate(axis=1, name=\"concate_pair_layer\")\n",
    "    coatten_layer = RemappedCoAttentionWeight(atten_dim, name=\"coattention_weights_layer\")\n",
    "    featnorm_layer1 = FeatureNormalization(name=\"normalized_coattention_weights_layer1\", axis=1)\n",
    "    featnorm_layer2 = FeatureNormalization(name=\"normalized_coattention_weights_layer2\", axis=2)\n",
    "    focus_layer1 = layers.Dot((1, 1), name=\"focus_layer1\")\n",
    "    focus_layer2 = layers.Dot((2, 1), name=\"focus_layer2\")\n",
    "    pair_layer1 = layers.Concatenate(axis=-1, name=\"pair_layer1\")\n",
    "    pair_layer2 = layers.Concatenate(axis=-1, name=\"pair_layer2\")\n",
    "    \n",
    "    pairs = list()\n",
    "    \n",
    "    pairs += list(coatten_compare(\n",
    "        feature_concat_layer, coatten_layer,\n",
    "        featnorm_layer1, featnorm_layer2, \n",
    "        focus_layer1, focus_layer2, \n",
    "        pair_layer1, pair_layer2, \n",
    "        x1[0], x1[1], x2[0]))\n",
    "    \n",
    "    pairs += list(coatten_compare(\n",
    "        feature_concat_layer, coatten_layer,\n",
    "        featnorm_layer1, featnorm_layer2, \n",
    "        focus_layer1, focus_layer2, \n",
    "        pair_layer1, pair_layer2, \n",
    "        x1[0], x1[2], x2[1]))\n",
    "    \n",
    "    x = layers.Concatenate(axis=1, name=\"concate_layer\")(pairs)\n",
    "    x = layers.TimeDistributed(layers.Dropout(rate=drop_out, name=\"pair_dropout_layer\"))(x)\n",
    "    x = layers.TimeDistributed(layers.Dense(mlp_dim, name=\"pair_feature_map_layer\", activation=\"relu\"))(x)\n",
    "    x = layers.Flatten(name=\"pair_feature_flatten_layer1\")(x)\n",
    "    \n",
    "#     pooled_outputs = []\n",
    "#     for i in range(len(filter_sizes)):\n",
    "#         conv = layers.Conv1D(num_filters[i], kernel_size=filter_sizes[i], padding=padding, activation='relu')(x)\n",
    "#         if pooling == 'max':\n",
    "#             conv = layers.GlobalMaxPooling1D(name='global_pooling_layer' + str(i))(conv)\n",
    "#         else:\n",
    "#             conv = layers.GlobalAveragePooling1D(name='global_pooling_layer' + str(i))(conv)\n",
    "#         pooled_outputs.append(conv)\n",
    "#     if len(pooled_outputs) > 1:\n",
    "#         x = layers.Concatenate(name='concated_layer')(pooled_outputs)\n",
    "#     else:\n",
    "#         x = conv\n",
    "    \n",
    "    # MLP Layers\n",
    "    x = layers.BatchNormalization(name='batch_norm_layer')(x)\n",
    "    x = layers.Dropout(rate=drop_out, name=\"dropout_layer\")(x)\n",
    "        \n",
    "    for i in range(mlp_depth - 1):\n",
    "        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n",
    "        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n",
    "\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n",
    "\n",
    "    model = models.Model(inputs1 + inputs2, outputs)\n",
    "\n",
    "    if return_customized_layers:\n",
    "        return model, {'RemappedCoAttentionWeight': RemappedCoAttentionWeight,\n",
    "                       \"FeatureNormalization\": FeatureNormalization}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import callbacks as kc\n",
    "from keras import optimizers as ko\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "\n",
    "\n",
    "histories = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature_channels1 = 3\n",
    "num_feature_channels2 = 2\n",
    "\n",
    "num_embed_features = 11\n",
    "num_features1 = num_embed_features\n",
    "num_features2 = num_pos_features\n",
    "feature_dim1 = embed_dim\n",
    "output_dim = 3\n",
    "model_dim = 10 \n",
    "mlp_dim = 60\n",
    "mlp_depth=1\n",
    "drop_out=0.5\n",
    "return_customized_layers=True\n",
    "\n",
    "model, co_mlp = build_mlp_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    model_dim, mlp_dim, mlp_depth, drop_out, return_customized_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ 1.90970004e-02,  2.31189996e-01, -1.68099999e-01, ...,\n",
       "           5.23289979e-01,  1.47510007e-01, -1.68540001e-01],\n",
       "         [-2.28689998e-01, -2.95120001e-01, -5.32890022e-01, ...,\n",
       "           1.63240004e-02,  5.38100004e-02,  3.75319988e-01],\n",
       "         [ 1.90970004e-02,  2.31189996e-01, -1.68099999e-01, ...,\n",
       "           5.23289979e-01,  1.47510007e-01, -1.68540001e-01],\n",
       "         ...,\n",
       "         [-1.73738021e-02,  5.88684008e-02, -1.33412004e-01, ...,\n",
       "          -5.08671999e-02,  8.44199955e-02, -1.28687814e-01],\n",
       "         [-1.94714800e-01,  8.82460028e-02, -2.69323528e-01, ...,\n",
       "          -1.02137208e-01, -1.72906011e-01, -1.24619462e-01],\n",
       "         [-3.43275368e-02,  9.08697173e-02, -1.06396154e-01, ...,\n",
       "          -1.44076362e-01, -1.49203360e-01, -1.88951030e-01]],\n",
       " \n",
       "        [[-2.23059997e-01,  1.54560000e-01, -3.21319997e-01, ...,\n",
       "           3.91719997e-01,  2.63280004e-01,  1.76139995e-02],\n",
       "         [ 3.13919991e-01,  4.57060002e-02,  2.95439996e-02, ...,\n",
       "          -2.24869996e-01,  1.67750001e-01,  2.22819999e-01],\n",
       "         [ 3.75729986e-02,  3.72200012e-01,  2.69910008e-01, ...,\n",
       "           3.48369986e-01, -1.72849998e-01, -7.16539994e-02],\n",
       "         ...,\n",
       "         [ 2.33887974e-02,  9.04233307e-02, -6.05244637e-02, ...,\n",
       "          -1.66329414e-01,  2.47826010e-01, -1.10358000e-01],\n",
       "         [ 5.18400362e-03,  1.41299993e-01, -1.64298013e-01, ...,\n",
       "           2.40625981e-02, -7.46426433e-02,  7.30028078e-02],\n",
       "         [-2.68858783e-02,  9.24703255e-02, -3.27295624e-02, ...,\n",
       "          -1.55021191e-01,  6.96558878e-02, -4.09838110e-02]],\n",
       " \n",
       "        [[ 8.51809978e-02,  5.08920014e-01, -8.82799998e-02, ...,\n",
       "           4.36789989e-01,  2.97839999e-01, -8.81500021e-02],\n",
       "         [-4.76870015e-02,  3.34689990e-02,  1.89070001e-01, ...,\n",
       "          -5.68809994e-02, -1.07780002e-01, -4.89289999e-01],\n",
       "         [ 8.51809978e-02,  5.08920014e-01, -8.82799998e-02, ...,\n",
       "           4.36789989e-01,  2.97839999e-01, -8.81500021e-02],\n",
       "         ...,\n",
       "         [-2.09415965e-02,  1.15786396e-01,  1.52231991e-01, ...,\n",
       "          -9.41927880e-02,  6.02081940e-02, -6.93978071e-02],\n",
       "         [-1.74001995e-02,  2.33578205e-01, -2.37760004e-02, ...,\n",
       "           1.97285786e-01,  3.99577990e-02, -9.55934301e-02],\n",
       "         [-4.96156886e-02,  1.18819110e-01, -4.61722054e-02, ...,\n",
       "           1.48694098e-01, -2.57845782e-02,  6.98748976e-02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 2.36479998e-01,  3.90910000e-01, -9.58020017e-02, ...,\n",
       "           1.32280007e-01,  3.64170015e-01, -2.86760002e-01],\n",
       "         [-1.47119999e-01,  2.07580000e-01, -2.15890005e-01, ...,\n",
       "           1.07380003e-01,  7.07430020e-02,  4.33299989e-01],\n",
       "         [ 2.36479998e-01,  3.90910000e-01, -9.58020017e-02, ...,\n",
       "           1.32280007e-01,  3.64170015e-01, -2.86760002e-01],\n",
       "         ...,\n",
       "         [-9.99002978e-02,  1.49634928e-01, -2.10580025e-02, ...,\n",
       "          -1.14160001e-01,  1.51345998e-01,  2.55712029e-02],\n",
       "         [ 7.03150406e-02,  2.23245814e-01, -1.32638603e-01, ...,\n",
       "          -1.45585984e-01,  1.01134613e-01,  1.48651391e-01],\n",
       "         [ 1.14322836e-02,  1.65223867e-01, -4.44054650e-03, ...,\n",
       "          -8.59429315e-02, -6.25890791e-02, -7.93253183e-02]],\n",
       " \n",
       "        [[ 2.36479998e-01,  3.90910000e-01, -9.58020017e-02, ...,\n",
       "           1.32280007e-01,  3.64170015e-01, -2.86760002e-01],\n",
       "         [ 3.78670007e-01, -3.54299992e-01, -1.33080006e-01, ...,\n",
       "           1.61569998e-01, -4.03780013e-01,  1.87350005e-01],\n",
       "         [ 2.06900001e-01,  4.43210006e-01, -1.25220001e-01, ...,\n",
       "           1.94330007e-01, -9.35930014e-02, -8.97829980e-02],\n",
       "         ...,\n",
       "         [ 3.39425981e-01, -6.92120045e-02, -2.21455976e-01, ...,\n",
       "          -1.14040049e-02, -1.76062137e-01,  8.17603990e-02],\n",
       "         [ 1.82621390e-01,  7.21840039e-02, -1.08310416e-01, ...,\n",
       "           7.37650096e-02, -5.37372008e-02,  1.55818582e-01],\n",
       "         [ 2.11632684e-01,  5.91876432e-02, -5.54326400e-02, ...,\n",
       "           1.92447007e-02, -9.85042602e-02, -3.70781240e-03]],\n",
       " \n",
       "        [[ 1.42030001e-01,  9.50829983e-02, -2.68099993e-01, ...,\n",
       "           1.37360007e-01,  1.57399997e-01, -3.53289992e-01],\n",
       "         [ 3.95700008e-01, -1.20360002e-01,  3.82609993e-01, ...,\n",
       "           1.03170000e-01, -2.95379996e-01,  3.87479991e-01],\n",
       "         [-3.40640008e-01,  6.54479980e-01,  1.97200011e-03, ...,\n",
       "           8.81730020e-02, -2.67490000e-01, -2.25250006e-01],\n",
       "         ...,\n",
       "         [-4.64024022e-02,  1.93413585e-01, -8.13786015e-02, ...,\n",
       "          -1.01586796e-01, -1.12782799e-01,  9.67859849e-02],\n",
       "         [ 1.09946202e-01,  3.64465982e-02, -2.25400031e-03, ...,\n",
       "           7.58480027e-02, -9.96939987e-02, -1.62000209e-04],\n",
       "         [-2.88594328e-02,  2.48172238e-01, -4.08427641e-02, ...,\n",
       "          -5.60200773e-02, -1.06742360e-01, -1.44275259e-02]]]),\n",
       " array([[[ 6.07159995e-02,  1.69880003e-01, -2.21110001e-01, ...,\n",
       "          -2.37639993e-01,  2.97999997e-02, -2.31749997e-01],\n",
       "         [ 5.15510023e-01, -8.99309963e-02,  5.06020002e-02, ...,\n",
       "          -2.03400001e-01, -3.67870003e-01,  4.80809994e-02],\n",
       "         [ 4.48579997e-01, -1.73659995e-03, -2.50600010e-01, ...,\n",
       "          -4.21950012e-01, -3.64080012e-01, -1.57919992e-02],\n",
       "         ...,\n",
       "         [ 3.04913998e-01,  2.14438394e-01,  4.97843996e-02, ...,\n",
       "          -3.27992409e-01,  1.00225784e-01,  2.13284180e-01],\n",
       "         [ 6.18112013e-02,  9.63662043e-02, -8.09016079e-02, ...,\n",
       "          -2.24067211e-01,  4.58999984e-02, -1.39631793e-01],\n",
       "         [ 1.42282471e-01,  1.28269091e-01, -6.78117350e-02, ...,\n",
       "          -2.11761862e-01,  1.79251037e-05,  2.32420620e-02]],\n",
       " \n",
       "        [[ 2.89570000e-02,  5.10500014e-01,  2.19689995e-01, ...,\n",
       "          -3.59899998e-01,  3.24440002e-01, -1.41379997e-01],\n",
       "         [ 1.61550000e-01, -6.85259998e-01,  4.73289996e-01, ...,\n",
       "           6.83619976e-02,  3.61909986e-01,  8.54849964e-02],\n",
       "         [ 3.75729986e-02,  3.72200012e-01,  2.69910008e-01, ...,\n",
       "           3.48369986e-01, -1.72849998e-01, -7.16539994e-02],\n",
       "         ...,\n",
       "         [-1.09696604e-01,  2.56580800e-01, -9.19719934e-02, ...,\n",
       "          -2.39707202e-01,  1.71550602e-01, -6.66600019e-02],\n",
       "         [-3.11853942e-02,  1.18908007e-02, -6.38599973e-03, ...,\n",
       "          -1.35831401e-01,  2.64691174e-01, -6.46300334e-03],\n",
       "         [-2.68858783e-02,  9.24703255e-02, -3.27295624e-02, ...,\n",
       "          -1.55021191e-01,  6.96558878e-02, -4.09838110e-02]],\n",
       " \n",
       "        [[-2.16549993e-01, -1.51789993e-01,  6.18059993e-01, ...,\n",
       "          -5.95870020e-04, -2.25759998e-01,  1.02420002e-01],\n",
       "         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 4.37979996e-02,  2.47789994e-02, -2.09370002e-01, ...,\n",
       "          -3.00989985e-01, -1.45840004e-01,  2.81879991e-01],\n",
       "         ...,\n",
       "         [-2.78323945e-02,  2.57009208e-01,  2.17209980e-01, ...,\n",
       "           8.34958106e-02, -2.14899778e-01,  1.10995993e-01],\n",
       "         [-1.55084012e-02,  3.04674022e-02, -6.24240041e-02, ...,\n",
       "          -4.39035706e-02, -9.98100042e-02,  1.16211995e-01],\n",
       "         [-1.88086387e-02,  9.16928425e-02,  6.71665296e-02, ...,\n",
       "          -6.10238388e-02, -9.34304222e-02,  7.44121894e-02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 1.78989992e-02,  2.69580007e-01, -2.79389992e-02, ...,\n",
       "           3.28349993e-02, -7.12629974e-01, -4.84539986e-01],\n",
       "         [-1.06480002e-01, -1.62950009e-02, -2.27550000e-01, ...,\n",
       "          -3.13430011e-01,  8.74240026e-02, -1.66099995e-01],\n",
       "         [ 2.36479998e-01,  3.90910000e-01, -9.58020017e-02, ...,\n",
       "           1.32280007e-01,  3.64170015e-01, -2.86760002e-01],\n",
       "         ...,\n",
       "         [-2.66144015e-02,  1.45236790e-01, -1.52736217e-01, ...,\n",
       "          -8.74079987e-02, -5.90860099e-03,  2.15764001e-01],\n",
       "         [-2.37433966e-02,  1.95704013e-01, -1.24218035e-02, ...,\n",
       "          -1.43036991e-01, -3.65641981e-01,  6.46296069e-02],\n",
       "         [ 1.14322836e-02,  1.65223867e-01, -4.44054650e-03, ...,\n",
       "          -8.59429315e-02, -6.25890791e-02, -7.93253183e-02]],\n",
       " \n",
       "        [[ 2.18720004e-01,  6.32849991e-01,  4.24939990e-01, ...,\n",
       "          -3.29019994e-01, -5.87790012e-01, -5.80049992e-01],\n",
       "         [ 2.38319993e-01, -1.43519998e-01,  1.01040006e+00, ...,\n",
       "          -5.18890023e-01, -3.27289999e-01, -4.33499992e-01],\n",
       "         [ 2.06900001e-01,  4.43210006e-01, -1.25220001e-01, ...,\n",
       "           1.94330007e-01, -9.35930014e-02, -8.97829980e-02],\n",
       "         ...,\n",
       "         [ 1.60981789e-01,  5.14311969e-01,  1.19080041e-02, ...,\n",
       "           9.91580039e-02, -2.10607380e-01, -4.48726043e-02],\n",
       "         [ 1.31778806e-01,  1.16007999e-01,  1.83216006e-01, ...,\n",
       "          -1.71021998e-01, -1.88150018e-01, -5.31440005e-02],\n",
       "         [ 2.11632684e-01,  5.91876432e-02, -5.54326400e-02, ...,\n",
       "           1.92447007e-02, -9.85042602e-02, -3.70781240e-03]],\n",
       " \n",
       "        [[-3.40640008e-01,  6.54479980e-01,  1.97200011e-03, ...,\n",
       "           8.81730020e-02, -2.67490000e-01, -2.25250006e-01],\n",
       "         [-4.10089999e-01,  5.64989984e-01,  2.55710006e-01, ...,\n",
       "          -3.20160002e-01, -3.11190009e-01,  2.25419998e-02],\n",
       "         [-3.40640008e-01,  6.54479980e-01,  1.97200011e-03, ...,\n",
       "           8.81730020e-02, -2.67490000e-01, -2.25250006e-01],\n",
       "         ...,\n",
       "         [-8.93278047e-02,  1.86364204e-01, -2.53753990e-01, ...,\n",
       "          -8.68235976e-02, -1.55537799e-01,  2.56310016e-01],\n",
       "         [-1.75116211e-01,  4.21579987e-01, -8.70831385e-02, ...,\n",
       "          -1.68643400e-01, -9.20646042e-02, -6.52230605e-02],\n",
       "         [-2.88594328e-02,  2.48172238e-01, -4.08427641e-02, ...,\n",
       "          -5.60200773e-02, -1.06742360e-01, -1.44275259e-02]]]),\n",
       " array([[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 1.68300003e-01, -3.15050006e-01, -3.07280004e-01, ...,\n",
       "           2.86249995e-01,  2.15020001e-01, -9.39489976e-02],\n",
       "         [ 4.48579997e-01, -1.73659995e-03, -2.50600010e-01, ...,\n",
       "          -4.21950012e-01, -3.64080012e-01, -1.57919992e-02],\n",
       "         ...,\n",
       "         [ 1.80165201e-01,  2.25306198e-01, -5.62175997e-02, ...,\n",
       "          -3.33527982e-01,  1.37373000e-01, -7.27197975e-02],\n",
       "         [ 2.61636022e-02,  1.17670000e-01, -9.38040018e-02, ...,\n",
       "           1.70692801e-01,  5.81640117e-02, -1.09985806e-01],\n",
       "         [ 1.42282471e-01,  1.28269091e-01, -6.78117350e-02, ...,\n",
       "          -2.11761862e-01,  1.79251037e-05,  2.32420620e-02]],\n",
       " \n",
       "        [[-5.20290017e-01,  8.98720026e-02,  5.00670016e-01, ...,\n",
       "          -9.66830015e-01,  1.18019998e-01,  2.82770008e-01],\n",
       "         [-3.78989995e-01, -3.93310010e-01,  3.73679996e-01, ...,\n",
       "           2.54009992e-01, -2.84200013e-01,  4.15619999e-01],\n",
       "         [ 3.75729986e-02,  3.72200012e-01,  2.69910008e-01, ...,\n",
       "           3.48369986e-01, -1.72849998e-01, -7.16539994e-02],\n",
       "         ...,\n",
       "         [ 5.63972071e-02,  5.95507920e-02, -1.25903994e-01, ...,\n",
       "          -7.32765943e-02,  2.21411183e-01, -7.75509998e-02],\n",
       "         [-2.00982183e-01, -5.14147896e-03,  1.08399197e-01, ...,\n",
       "          -2.03549415e-01,  8.99999961e-02,  4.02720049e-02],\n",
       "         [-2.68858783e-02,  9.24703255e-02, -3.27295624e-02, ...,\n",
       "          -1.55021191e-01,  6.96558878e-02, -4.09838110e-02]],\n",
       " \n",
       "        [[-3.45759988e-01,  2.48419996e-02,  8.07969987e-01, ...,\n",
       "          -6.60629988e-01, -1.31540000e-01,  4.69509996e-02],\n",
       "         [-2.16090009e-02, -1.81280002e-01,  2.25409999e-01, ...,\n",
       "           6.29969984e-02,  4.06489998e-01, -3.56950015e-01],\n",
       "         [ 4.37979996e-02,  2.47789994e-02, -2.09370002e-01, ...,\n",
       "          -3.00989985e-01, -1.45840004e-01,  2.81879991e-01],\n",
       "         ...,\n",
       "         [ 1.88865587e-01,  2.88898051e-02, -1.74163193e-01, ...,\n",
       "          -3.48669924e-02, -5.03938086e-02,  1.80404112e-01],\n",
       "         [-6.77533969e-02,  2.04938412e-01,  1.90291986e-01, ...,\n",
       "          -2.33837962e-02,  1.00216009e-01, -1.37117818e-01],\n",
       "         [-1.88086387e-02,  9.16928425e-02,  6.71665296e-02, ...,\n",
       "          -6.10238388e-02, -9.34304222e-02,  7.44121894e-02]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 1.41039997e-01,  9.68219995e-01,  1.43600002e-01, ...,\n",
       "          -2.24680007e-01,  2.84630004e-02, -4.95979995e-01],\n",
       "         [ 2.40339991e-02,  6.00429997e-02,  5.00980020e-01, ...,\n",
       "          -1.91290006e-01, -5.96930012e-02, -6.59850001e-01],\n",
       "         [ 2.36479998e-01,  3.90910000e-01, -9.58020017e-02, ...,\n",
       "           1.32280007e-01,  3.64170015e-01, -2.86760002e-01],\n",
       "         ...,\n",
       "         [ 1.16216000e-02, -4.22822013e-02, -6.44599786e-03, ...,\n",
       "          -1.03315994e-01,  4.90460312e-03, -3.81884009e-01],\n",
       "         [ 4.13287990e-02,  3.07854384e-01,  2.08274007e-01, ...,\n",
       "           1.84839927e-02, -6.82999939e-02, -1.95532411e-01],\n",
       "         [ 1.14322836e-02,  1.65223867e-01, -4.44054650e-03, ...,\n",
       "          -8.59429315e-02, -6.25890791e-02, -7.93253183e-02]],\n",
       " \n",
       "        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "           0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "         [ 3.91310006e-01, -4.68699992e-01, -2.66970009e-01, ...,\n",
       "          -4.47490007e-01, -5.60880005e-01,  3.23630005e-01],\n",
       "         [ 2.06900001e-01,  4.43210006e-01, -1.25220001e-01, ...,\n",
       "           1.94330007e-01, -9.35930014e-02, -8.97829980e-02],\n",
       "         ...,\n",
       "         [ 1.15228400e-01,  2.50415981e-01,  1.53241992e-01, ...,\n",
       "          -2.09382012e-01, -2.63841987e-01, -6.63219914e-02],\n",
       "         [ 3.32313985e-01,  2.13760026e-02, -2.02936411e-01, ...,\n",
       "          -1.09780012e-02, -6.65941238e-02, -2.05159793e-03],\n",
       "         [ 2.11632684e-01,  5.91876432e-02, -5.54326400e-02, ...,\n",
       "           1.92447007e-02, -9.85042602e-02, -3.70781240e-03]],\n",
       " \n",
       "        [[ 3.84519994e-01,  4.89529997e-01, -6.53739989e-01, ...,\n",
       "           6.52659982e-02,  3.03359985e-01,  5.92599988e-01],\n",
       "         [-2.44650006e-01,  2.68479995e-02,  7.18859971e-01, ...,\n",
       "           2.30460003e-01, -4.97099996e-01,  1.10100001e-01],\n",
       "         [-3.40640008e-01,  6.54479980e-01,  1.97200011e-03, ...,\n",
       "           8.81730020e-02, -2.67490000e-01, -2.25250006e-01],\n",
       "         ...,\n",
       "         [-3.50380018e-02,  7.64579922e-02, -1.47168592e-01, ...,\n",
       "          -1.64070010e-01,  8.35993960e-02, -1.25852004e-01],\n",
       "         [ 1.18969604e-01,  2.32628182e-01,  5.95199456e-03, ...,\n",
       "           6.88912049e-02, -1.42035991e-01,  1.34200007e-01],\n",
       "         [-2.88594328e-02,  2.48172238e-01, -4.08427641e-02, ...,\n",
       "          -5.60200773e-02, -1.06742360e-01, -1.44275259e-02]]]),\n",
       " array([[1., 0., 0., ..., 1., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 1., 0.]]),\n",
       " array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 1.],\n",
       "        [1., 0., 0., ..., 1., 0., 0.]])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 454 samples\n",
      "Epoch 1/20\n",
      "2000/2000 [==============================] - 6s 3ms/step - loss: 1.5324 - sparse_categorical_accuracy: 0.4225 - val_loss: 0.9507 - val_sparse_categorical_accuracy: 0.5485\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95066, saving model to best_mlp_model.hdf5\n",
      "Epoch 2/20\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 1.0857 - sparse_categorical_accuracy: 0.5430 - val_loss: 0.8557 - val_sparse_categorical_accuracy: 0.5991\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95066 to 0.85572, saving model to best_mlp_model.hdf5\n",
      "Epoch 3/20\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.9036 - sparse_categorical_accuracy: 0.5905 - val_loss: 0.8062 - val_sparse_categorical_accuracy: 0.6454\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.85572 to 0.80623, saving model to best_mlp_model.hdf5\n",
      "Epoch 4/20\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.8436 - sparse_categorical_accuracy: 0.6390 - val_loss: 0.7904 - val_sparse_categorical_accuracy: 0.6432\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.80623 to 0.79042, saving model to best_mlp_model.hdf5\n",
      "Epoch 5/20\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.8069 - sparse_categorical_accuracy: 0.6320 - val_loss: 0.7812 - val_sparse_categorical_accuracy: 0.6410\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.79042 to 0.78123, saving model to best_mlp_model.hdf5\n",
      "Epoch 6/20\n",
      "2000/2000 [==============================] - 1s 397us/step - loss: 0.7969 - sparse_categorical_accuracy: 0.6490 - val_loss: 0.7706 - val_sparse_categorical_accuracy: 0.6520\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.78123 to 0.77062, saving model to best_mlp_model.hdf5\n",
      "Epoch 7/20\n",
      "2000/2000 [==============================] - 1s 396us/step - loss: 0.7467 - sparse_categorical_accuracy: 0.6715 - val_loss: 0.7681 - val_sparse_categorical_accuracy: 0.6476\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.77062 to 0.76814, saving model to best_mlp_model.hdf5\n",
      "Epoch 8/20\n",
      "2000/2000 [==============================] - 1s 400us/step - loss: 0.7878 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.7660 - val_sparse_categorical_accuracy: 0.6366\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.76814 to 0.76596, saving model to best_mlp_model.hdf5\n",
      "Epoch 9/20\n",
      "2000/2000 [==============================] - 1s 415us/step - loss: 0.7439 - sparse_categorical_accuracy: 0.6765 - val_loss: 0.7631 - val_sparse_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.76596 to 0.76311, saving model to best_mlp_model.hdf5\n",
      "Epoch 10/20\n",
      "2000/2000 [==============================] - 1s 402us/step - loss: 0.7318 - sparse_categorical_accuracy: 0.6700 - val_loss: 0.7666 - val_sparse_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.76311\n",
      "Epoch 11/20\n",
      "2000/2000 [==============================] - 1s 394us/step - loss: 0.7048 - sparse_categorical_accuracy: 0.6975 - val_loss: 0.7676 - val_sparse_categorical_accuracy: 0.6388\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.76311\n",
      "Epoch 12/20\n",
      "2000/2000 [==============================] - 1s 392us/step - loss: 0.7219 - sparse_categorical_accuracy: 0.6900 - val_loss: 0.7652 - val_sparse_categorical_accuracy: 0.6432\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.76311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam()\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_mlp_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=3)\n",
    "history = model.fit(X_train, y_tra, batch_size=20, epochs=20, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop])\n",
    "\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del model, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature_channels1 = 3\n",
    "num_feature_channels2 = 2\n",
    "\n",
    "num_embed_features = 11\n",
    "num_features1 = num_embed_features\n",
    "num_features2 = num_pos_features\n",
    "feature_dim1 = embed_dim\n",
    "output_dim = 3\n",
    "atten_dim = 10\n",
    "model_dim = 10\n",
    "filter_sizes = [1]\n",
    "num_filters = [20] * len(filter_sizes)\n",
    "mlp_dim = 5\n",
    "mlp_depth=1\n",
    "pooling='max'\n",
    "padding='valid'\n",
    "drop_out=0.5\n",
    "return_customized_layers=True\n",
    "\n",
    "model, co_cacnn = build_inter_coattention_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, atten_dim, model_dim, mlp_dim, mlp_depth, drop_out, pooling, padding, return_customized_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 454 samples\n",
      "Epoch 1/40\n",
      "2000/2000 [==============================] - 2s 843us/step - loss: 1.4880 - sparse_categorical_accuracy: 0.4190 - val_loss: 1.0012 - val_sparse_categorical_accuracy: 0.5639\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00118, saving model to best_coatt_cnn_model.hdf5\n",
      "Epoch 2/40\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 1.1249 - sparse_categorical_accuracy: 0.5110 - val_loss: 0.8964 - val_sparse_categorical_accuracy: 0.6278\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00118 to 0.89642, saving model to best_coatt_cnn_model.hdf5\n",
      "Epoch 3/40\n",
      "2000/2000 [==============================] - 1s 326us/step - loss: 0.9551 - sparse_categorical_accuracy: 0.5735 - val_loss: 0.8592 - val_sparse_categorical_accuracy: 0.6145\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.89642 to 0.85922, saving model to best_coatt_cnn_model.hdf5\n",
      "Epoch 4/40\n",
      "2000/2000 [==============================] - 1s 324us/step - loss: 0.8664 - sparse_categorical_accuracy: 0.6275 - val_loss: 0.8325 - val_sparse_categorical_accuracy: 0.6233\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.85922 to 0.83250, saving model to best_coatt_cnn_model.hdf5\n",
      "Epoch 5/40\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 0.8095 - sparse_categorical_accuracy: 0.6505 - val_loss: 0.8087 - val_sparse_categorical_accuracy: 0.6388\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.83250 to 0.80869, saving model to best_coatt_cnn_model.hdf5\n",
      "Epoch 6/40\n",
      "2000/2000 [==============================] - 1s 330us/step - loss: 0.7696 - sparse_categorical_accuracy: 0.6615 - val_loss: 0.7998 - val_sparse_categorical_accuracy: 0.6278\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.80869 to 0.79980, saving model to best_coatt_cnn_model.hdf5\n",
      "Epoch 7/40\n",
      "2000/2000 [==============================] - 1s 324us/step - loss: 0.7739 - sparse_categorical_accuracy: 0.6600 - val_loss: 0.7826 - val_sparse_categorical_accuracy: 0.6520\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.79980 to 0.78257, saving model to best_coatt_cnn_model.hdf5\n",
      "Epoch 8/40\n",
      "2000/2000 [==============================] - 1s 325us/step - loss: 0.7439 - sparse_categorical_accuracy: 0.6770 - val_loss: 0.7866 - val_sparse_categorical_accuracy: 0.6410\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.78257\n",
      "Epoch 9/40\n",
      "2000/2000 [==============================] - 1s 332us/step - loss: 0.7251 - sparse_categorical_accuracy: 0.6745 - val_loss: 0.7847 - val_sparse_categorical_accuracy: 0.6366\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.78257\n",
      "Epoch 10/40\n",
      "2000/2000 [==============================] - 1s 320us/step - loss: 0.7186 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.7829 - val_sparse_categorical_accuracy: 0.6344\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.78257\n",
      "Epoch 11/40\n",
      "2000/2000 [==============================] - 1s 324us/step - loss: 0.6991 - sparse_categorical_accuracy: 0.6955 - val_loss: 0.7787 - val_sparse_categorical_accuracy: 0.6432\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.78257 to 0.77872, saving model to best_coatt_cnn_model.hdf5\n",
      "Epoch 12/40\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.6783 - sparse_categorical_accuracy: 0.6990 - val_loss: 0.7825 - val_sparse_categorical_accuracy: 0.6256\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.77872\n",
      "Epoch 13/40\n",
      "2000/2000 [==============================] - 1s 319us/step - loss: 0.6683 - sparse_categorical_accuracy: 0.7170 - val_loss: 0.8018 - val_sparse_categorical_accuracy: 0.6057\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.77872\n",
      "Epoch 14/40\n",
      "2000/2000 [==============================] - 1s 321us/step - loss: 0.6574 - sparse_categorical_accuracy: 0.7160 - val_loss: 0.7923 - val_sparse_categorical_accuracy: 0.6366\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.77872\n",
      "Epoch 15/40\n",
      "2000/2000 [==============================] - 1s 328us/step - loss: 0.6457 - sparse_categorical_accuracy: 0.7280 - val_loss: 0.7955 - val_sparse_categorical_accuracy: 0.6322\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.77872\n",
      "Epoch 16/40\n",
      "2000/2000 [==============================] - 1s 323us/step - loss: 0.6332 - sparse_categorical_accuracy: 0.7285 - val_loss: 0.8073 - val_sparse_categorical_accuracy: 0.6256\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.77872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam()\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_coatt_cnn_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=5)\n",
    "history = model.fit(X_train, y_tra, batch_size=30, epochs=40, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop])\n",
    "\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del model, history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature_channels1 = 3\n",
    "num_feature_channels2 = 2\n",
    "\n",
    "num_embed_features = 11\n",
    "num_features1 = num_embed_features\n",
    "num_features2 = num_pos_features\n",
    "feature_dim1 = embed_dim\n",
    "output_dim = 3\n",
    "atten_dim = 10\n",
    "model_dim = 10\n",
    "filter_sizes = [1]\n",
    "num_filters = [20] * len(filter_sizes)\n",
    "mlp_dim = 5\n",
    "mlp_depth=1\n",
    "pooling='max'\n",
    "padding='valid'\n",
    "drop_out=0.5\n",
    "return_customized_layers=True\n",
    "\n",
    "model, intra_co_cacnn = build_intra_coattention_cnn_model(\n",
    "    num_feature_channels1, num_feature_channels2, num_features1, num_features2, feature_dim1, output_dim, \n",
    "    num_filters, filter_sizes, atten_dim, model_dim, mlp_dim, mlp_depth, drop_out, pooling, padding, return_customized_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 454 samples\n",
      "Epoch 1/40\n",
      "2000/2000 [==============================] - 2s 914us/step - loss: 1.5341 - sparse_categorical_accuracy: 0.4160 - val_loss: 0.9886 - val_sparse_categorical_accuracy: 0.5859\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.98861, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "Epoch 2/40\n",
      "2000/2000 [==============================] - 1s 363us/step - loss: 1.0939 - sparse_categorical_accuracy: 0.5600 - val_loss: 0.9032 - val_sparse_categorical_accuracy: 0.6322\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.98861 to 0.90319, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "Epoch 3/40\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.9093 - sparse_categorical_accuracy: 0.6180 - val_loss: 0.8493 - val_sparse_categorical_accuracy: 0.6189\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.90319 to 0.84928, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "Epoch 4/40\n",
      "2000/2000 [==============================] - 1s 361us/step - loss: 0.8721 - sparse_categorical_accuracy: 0.6215 - val_loss: 0.8202 - val_sparse_categorical_accuracy: 0.6233\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.84928 to 0.82018, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "Epoch 5/40\n",
      "2000/2000 [==============================] - 1s 368us/step - loss: 0.7979 - sparse_categorical_accuracy: 0.6520 - val_loss: 0.7956 - val_sparse_categorical_accuracy: 0.6145\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.82018 to 0.79561, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "Epoch 6/40\n",
      "2000/2000 [==============================] - 1s 364us/step - loss: 0.7533 - sparse_categorical_accuracy: 0.6640 - val_loss: 0.7849 - val_sparse_categorical_accuracy: 0.6233\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.79561 to 0.78492, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "Epoch 7/40\n",
      "2000/2000 [==============================] - 1s 374us/step - loss: 0.7298 - sparse_categorical_accuracy: 0.6820 - val_loss: 0.7753 - val_sparse_categorical_accuracy: 0.6366\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.78492 to 0.77534, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "Epoch 8/40\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.7308 - sparse_categorical_accuracy: 0.6810 - val_loss: 0.7682 - val_sparse_categorical_accuracy: 0.6256\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.77534 to 0.76815, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "Epoch 9/40\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.7201 - sparse_categorical_accuracy: 0.6860 - val_loss: 0.7713 - val_sparse_categorical_accuracy: 0.6388\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.76815\n",
      "Epoch 10/40\n",
      "2000/2000 [==============================] - 1s 371us/step - loss: 0.6768 - sparse_categorical_accuracy: 0.7060 - val_loss: 0.7677 - val_sparse_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.76815 to 0.76769, saving model to best_intra_coatt_cnn_model.hdf5\n",
      "Epoch 11/40\n",
      "2000/2000 [==============================] - 1s 373us/step - loss: 0.6877 - sparse_categorical_accuracy: 0.7045 - val_loss: 0.8209 - val_sparse_categorical_accuracy: 0.6145\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.76769\n",
      "Epoch 12/40\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.6690 - sparse_categorical_accuracy: 0.7085 - val_loss: 0.7700 - val_sparse_categorical_accuracy: 0.6498\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.76769\n",
      "Epoch 13/40\n",
      "2000/2000 [==============================] - 1s 369us/step - loss: 0.6562 - sparse_categorical_accuracy: 0.7250 - val_loss: 0.7828 - val_sparse_categorical_accuracy: 0.6300\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.76769\n",
      "Epoch 14/40\n",
      "2000/2000 [==============================] - 1s 362us/step - loss: 0.6140 - sparse_categorical_accuracy: 0.7365 - val_loss: 0.8481 - val_sparse_categorical_accuracy: 0.6013\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.76769\n",
      "Epoch 15/40\n",
      "2000/2000 [==============================] - 1s 372us/step - loss: 0.6217 - sparse_categorical_accuracy: 0.7325 - val_loss: 0.8483 - val_sparse_categorical_accuracy: 0.5969\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.76769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = ko.Nadam()\n",
    "model.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "file_path = \"best_intra_coatt_cnn_model.hdf5\"\n",
    "check_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n",
    "early_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=5)\n",
    "history = model.fit(X_train, y_tra, batch_size=30, epochs=40, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop])\n",
    "\n",
    "histories.append(np.min(np.asarray(history.history['val_loss'])))\n",
    "\n",
    "del history\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'co_mccnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-244b10a7dba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m cls_ =[\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mco_mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mco_mccnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mco_cacnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintra_co_cacnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'co_mccnn' is not defined"
     ]
    }
   ],
   "source": [
    "model_paths = [\n",
    "    \"best_mlp_model.hdf5\",\n",
    "    \"best_mc_cnn_model.hdf5\",\n",
    "    \"best_coatt_cnn_model.hdf5\",\n",
    "    \"best_intra_coatt_cnn_model.hdf5\"\n",
    "]\n",
    "\n",
    "cls_ =[\n",
    "    co_mlp, co_mccnn, co_cacnn, intra_co_cacnn\n",
    "]\n",
    "\n",
    "print(\"load best model: \" + str(model_paths[np.argmin(histories)]))\n",
    "model = models.load_model(\n",
    "    model_paths[np.argmin(histories)], cls_[np.argmin(histories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best_mlp_model.hdf5'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(model_paths[np.argmin(histories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 204us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>NEITHER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>0.175153</td>\n",
       "      <td>0.503326</td>\n",
       "      <td>0.321521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>0.990665</td>\n",
       "      <td>0.007433</td>\n",
       "      <td>0.001902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>0.120768</td>\n",
       "      <td>0.870317</td>\n",
       "      <td>0.008915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>0.168025</td>\n",
       "      <td>0.336803</td>\n",
       "      <td>0.495172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>0.227451</td>\n",
       "      <td>0.241365</td>\n",
       "      <td>0.531183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         A         B   NEITHER\n",
       "0  development-1  0.175153  0.503326  0.321521\n",
       "1  development-2  0.990665  0.007433  0.001902\n",
       "2  development-3  0.120768  0.870317  0.008915\n",
       "3  development-4  0.168025  0.336803  0.495172\n",
       "4  development-5  0.227451  0.241365  0.531183"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = model.predict(X_test, batch_size = 1024, verbose = 1)\n",
    "\n",
    "#sub_df_path = os.path.join(\"\")\n",
    "sub_df = pd.read_csv(\"../input/sample/sample_submission_stage_1.csv\")\n",
    "sub_df.loc[:, 'A'] = pd.Series(y_preds[:, 0])\n",
    "sub_df.loc[:, 'B'] = pd.Series(y_preds[:, 1])\n",
    "sub_df.loc[:, 'NEITHER'] = pd.Series(y_preds[:, 2])\n",
    "\n",
    "sub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.39189997e-01, -3.26279998e-02, -5.33919990e-01,  3.44850004e-01,\n",
       "        4.45200019e-02, -1.65260002e-01,  4.66479987e-01, -9.28759992e-01,\n",
       "        2.16199998e-02,  2.28329992e+00, -4.85489994e-01,  6.75539970e-02,\n",
       "       -2.62580007e-01, -2.04789996e-01, -3.90489995e-01,  1.43519998e-01,\n",
       "       -7.02230036e-02, -3.28920007e-01,  1.01729997e-01,  6.20480001e-01,\n",
       "        4.70510013e-02, -3.76170009e-01, -4.06269997e-01,  1.33499995e-01,\n",
       "        1.07309997e-01,  5.76489985e-01,  2.40970001e-01, -1.16779998e-01,\n",
       "       -1.20350003e-01, -2.10159998e-02,  1.29759997e-01,  3.17909986e-01,\n",
       "        9.31700021e-02, -8.00689962e-03,  3.16850007e-01, -5.52720010e-01,\n",
       "        1.86159998e-01, -3.83569986e-01, -2.54560001e-02, -6.55290019e-03,\n",
       "        3.56000006e-01, -4.18260008e-01,  1.97459996e-01,  4.12519991e-01,\n",
       "       -4.58689988e-01,  3.02230000e-01, -2.18490005e-01, -2.05720007e-01,\n",
       "        1.93930000e-01,  1.82840005e-01,  9.77239981e-02, -3.53489995e-01,\n",
       "       -1.21280000e-01, -2.13809997e-01, -1.48719996e-01,  2.11160004e-01,\n",
       "        8.67899973e-03, -1.03759997e-01,  8.80689994e-02,  1.04280002e-01,\n",
       "        4.49479997e-01, -5.41220009e-01, -2.69639999e-01, -4.00750011e-01,\n",
       "        2.58709997e-01, -1.93049997e-01, -3.44449997e-01,  1.91269994e-01,\n",
       "       -3.94160002e-01,  3.59559990e-02, -2.31050000e-01, -3.71280015e-02,\n",
       "        1.98070005e-01,  3.73589993e-01, -5.53950012e-01,  2.95549989e-01,\n",
       "        2.05479994e-01, -3.38270009e-01, -1.35340005e-01, -4.98690009e-02,\n",
       "        1.17040001e-01, -1.31960005e-01,  8.75089988e-02, -5.18179983e-02,\n",
       "        3.21779996e-01, -1.81810006e-01,  1.08780003e+00, -3.77580002e-02,\n",
       "        3.90769988e-01,  5.34500003e-01, -4.76049989e-01,  7.87189975e-02,\n",
       "        1.17550001e-01, -1.39699996e-01,  2.73550004e-02, -1.87030002e-01,\n",
       "        9.35479999e-02, -1.38429999e-01, -5.61789989e-01, -1.23140000e-01,\n",
       "        1.65889993e-01,  4.89219993e-01,  5.07939994e-01,  9.54940021e-02,\n",
       "        8.37979987e-02, -1.07399998e-02,  1.59730002e-01, -1.00290002e-02,\n",
       "       -4.12149988e-02, -1.86680004e-01, -1.00950003e-01, -1.34079993e-01,\n",
       "       -3.00099999e-01,  4.91239995e-01, -7.28899986e-02,  1.14990003e-01,\n",
       "        5.15999973e-01, -3.71050000e-01, -8.48779979e-04,  2.51459986e-01,\n",
       "       -1.51629999e-01,  1.71820000e-01,  2.63069987e-01, -4.61919993e-01,\n",
       "        5.72560012e-01,  5.05280018e-01, -1.37930006e-01,  1.64869994e-01,\n",
       "       -8.61029997e-02,  1.50040001e-01, -3.99659991e-01,  2.12730005e-01,\n",
       "       -1.48599997e-01,  2.58830011e-01,  4.43670005e-01, -1.61349997e-01,\n",
       "       -2.81259995e-02,  2.16330007e-01, -9.08999965e-02, -8.89400020e-02,\n",
       "       -2.94569993e+00, -2.75189996e-01,  3.16100009e-03,  4.88880008e-01,\n",
       "       -5.56900024e-01, -2.36680001e-01, -2.82720000e-01, -1.30009994e-01,\n",
       "       -1.03030004e-01, -2.67109990e-01, -1.56420007e-01,  3.66189986e-01,\n",
       "       -4.56880003e-01,  4.13329989e-01,  1.29559994e-01,  3.81089985e-01,\n",
       "        3.78270000e-01, -5.30229986e-01, -2.32560001e-02, -3.58759999e-01,\n",
       "       -1.55399993e-01, -4.90069985e-01, -5.54729998e-02, -2.97600001e-01,\n",
       "       -1.94830000e-01, -2.32800007e-01, -1.14730000e-02,  4.72900003e-01,\n",
       "        2.84969985e-01,  3.35640013e-01, -3.36160004e-01,  4.79919985e-02,\n",
       "        6.57369971e-01,  3.27089995e-01, -7.61649981e-02, -9.99120027e-02,\n",
       "       -5.41809976e-01, -2.50239998e-01, -2.14200005e-01,  2.95549989e-01,\n",
       "        2.71019995e-01, -7.04739988e-01, -6.08579993e-01,  4.74689990e-01,\n",
       "       -2.51020014e-01,  3.48690003e-01, -1.60170004e-01,  3.61919999e-01,\n",
       "        5.94870001e-02, -5.23370028e-01, -3.98949999e-03, -1.50979999e-02,\n",
       "       -5.63809991e-01,  9.05269980e-02,  1.37950003e-01, -7.49019980e-01,\n",
       "        9.47209969e-02,  5.47580011e-02,  4.50909995e-02,  7.67500028e-02,\n",
       "       -7.54630029e-01, -2.23110005e-01, -2.19190001e-01, -3.31860006e-01,\n",
       "        3.69670019e-02,  1.56059995e-01,  2.39010006e-01,  1.48169994e-01,\n",
       "       -1.72720000e-01,  4.81310003e-02, -4.27819997e-01, -1.33320004e-01,\n",
       "       -1.48650005e-01, -8.90100002e-02,  1.81309998e-01, -1.76019996e-01,\n",
       "       -7.14770034e-02, -1.90469995e-01,  1.49110004e-01, -1.96590006e-01,\n",
       "       -9.04280022e-02,  2.41909996e-02, -2.47620001e-01, -1.18529998e-01,\n",
       "       -9.62530002e-02,  1.51930004e-01,  2.09590003e-01, -7.33499974e-02,\n",
       "        3.39059997e-03,  2.14139998e-01, -3.13499987e-01,  6.85449988e-02,\n",
       "        6.32000029e-01,  5.76569978e-03,  3.28250006e-02, -4.95539993e-01,\n",
       "       -2.35850006e-01,  4.23310012e-01,  2.79790014e-01,  7.87999988e-01,\n",
       "        4.05790001e-01, -1.10940002e-01, -2.76959985e-01,  8.78079981e-02,\n",
       "        3.74659985e-01,  3.67729992e-01,  2.55140007e-01, -3.99989992e-01,\n",
       "        4.59380001e-01,  4.16860014e-01, -5.09480000e-01, -3.87790017e-02,\n",
       "        2.28809997e-01,  2.36970007e-01, -7.87530020e-02, -2.30439994e-02,\n",
       "       -1.36690000e-02, -4.52950001e-01, -5.86839989e-02, -8.53409991e-02,\n",
       "       -1.41619995e-01,  4.88240004e-01,  7.27739977e-03, -3.84350002e-01,\n",
       "       -2.46680006e-01,  6.22189999e-01, -4.14700001e-01,  3.42999995e-01,\n",
       "       -8.61310005e-01, -4.57379997e-01, -4.83790010e-01, -2.99659997e-01,\n",
       "       -3.08470011e-01,  6.48890018e-01, -3.52049991e-02, -2.84480006e-01,\n",
       "       -5.87199986e-01,  1.91060007e-01,  1.61259994e-01,  3.59329998e-01,\n",
       "        1.64480001e-01, -1.75310001e-01,  2.65350014e-01,  3.52869998e-03,\n",
       "        6.48669973e-02,  3.66930008e-01, -1.09320000e-01,  2.70839989e-01,\n",
       "       -4.51110005e-01,  1.66500002e-01,  6.35569990e-01,  1.28179997e-01,\n",
       "        1.43250003e-01,  3.06479990e-01,  5.56330025e-01, -7.28690028e-02,\n",
       "        7.06090033e-02,  2.22499996e-01,  1.40000001e-01, -2.49070004e-01])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0][0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
